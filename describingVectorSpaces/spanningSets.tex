\documentclass{ximera}
\input{../preamble.tex}
\title{Spanning sets, row spaces, and column spaces}
\author{Crichton Ogle}

\begin{document}
\begin{abstract}
  A collection of vectors spans a set if every vector in the set can
  be expressed as a linear combination of the vectors in the
  collection. The set of rows or columns of a matrix are spanning sets for the row and column space of the matrix.
\end{abstract}
\maketitle

If $S = \{{\bf v}_1,\dots, {\bf v}_n\}\subset V$ is a (finite) collection of vectors in a vector space $V$, then the {\it span} of $S$ is the set of all linear combinations of the vectors in $S$. That is
\[
Span(S) := \{\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2 +\dots +\alpha_n{\bf v}_n\ |\ \alpha_i\in\mathbb R\}
\] 

\begin{remark} If $S = \{{\bf v}_i\ |\ i\in\mathbb N\}$ is a countably infinite set of vectors, then the (linear, algebraic) span of the vectors is defined to be
\[
Span\{{\bf v}_i\ |\ i\in\mathbb N\} := \left\{\sum_i\alpha_i{\bf v}_i\ |\ \text{all but finitely many of the }\alpha_i\text{ are zero}\right\}
\]
the definition can be extended to arbitrarily large sets of vectors using a slightly different method of extension. Thus, if $S\subseteq V$ is an arbitrary set of vectors in $V$, then
\[
Span(S) := \underset{T\text{ finite}}{\underset{T\subseteq S}{\bigcup}} Span(T)
\]
\end{remark}

\begin{definition} If $V$ is a vector space, and $S$ a set of vectors in $V$, then we say that $S$ is a {\it spanning set} for $V$ if $V = Span(S)$.
\end{definition}

\begin{exercise} Show that for any (non-empty) set of vectors $S\subset V$, $Span(S)$ is a subset of $V$ (in other words, it is closed under the addition and scalar multiplication operations coming from $V$). Your argument should work for general sets $S$ without any assumptions on cardinality.
\end{exercise}

In fact, something stronger is true.

\begin{theorem} If $S$ is a non-empty subset of vectors in a vector space $V$, then $Span(S)$ is a subspace of $V$.
\end{theorem}

\begin{proof} Assume first that $S$ is finite; then $S = \{ {\bf v}_1,\dots,{\bf v}_n\}$ for some collection of vectors $\{{\bf v}\}_i\in V$, $n\ge 1$. To show that $Span(S)$ is a subspace, it suffices to show three things: i) it contains the zero vector, ii) it is closed under vector addition, and iii) it is closed under scalar multiplication.
\begin{itemize}
\item ${\bf z} = 0{\bf v}_1$, so ${\bf z}\in Span(S)$. In other words, the zero vector can be written as a linear ``combination" of a single vector in $S$ (a linear combination of one vector amounts to a scalar multiple of that vector).
\item {\bf\underbar{C1}} Suppose ${\bf v}, {\bf w}\in Span(S)$. Then there must exist scalars $\alpha_i, \beta_j\in \mathbb R$ such that
\[
{\bf v} = \alpha_1{\bf v}_1 +\dots \alpha_n{\bf v}_n,\qquad {\bf w} = \beta_1{\bf w}_1+\dots \beta_n{\bf w}_n
\]
then
\[
{\bf v} + {\bf w} = (\alpha_1 + \beta_1){\bf v}_1 + (\alpha_2 + \beta_2){\bf v}_2 +\dots + (\alpha_n+\beta_n){\bf v_n}\in Span(S)
\]
implying $Span(S)$ is closed under vector addition.
\item {\bf\underbar{C2}} For ${\bf v}$ as above and $\alpha\in\mathbb R$,
\[
\alpha{\bf v} = \alpha(\alpha_1{\bf v}_1 + \dots + \alpha_n{\bf v}_n) = (\alpha\alpha_1){\bf v}_1 + \dots (\alpha\alpha_n){\bf v}_n\in Span(S)
\]
Hence $Span(S)$ is closed under scalar multiplication.
\end{itemize}

Suppose now that $S$ is an infinite set. The first property is verified the same way, by choosing any vector ${\bf v}_1\in S$ and taking the scalar product with $0$. For the second property, for any ${\bf v},{\bf w}\in Span(S)$, there must be some finite set $T\subset S$ satisfying the property that ${\bf v}, {\bf w}\in Span(T)$. But then by the above argument we have that ${\bf v}+ {\bf w}\in Span(T)\subset Span(S)$, verifying the closure axiom (C1). Finally, for the same $\bf v$ and finite set $T\subset S$ with ${\bf v}\in Span(T)$, and scalar $\alpha\in\mathbb R$, the above argument shows that $\alpha{\bf v}\in Span(T)\subset Span(S)$, verifying the second closure axiom (C2). So the necessary properties have been verified, and we may conclude that $Span(S)$ is a subspace for any non-empty subset $S\subset V$, as claimed.

\end{proof}




\begin{lemma} Every vector space $V$ has a spanning set.
\end{lemma}

\begin{proof} Because we allow spanning sets to be arbitrarily large, we can take $S = V$ and observe that $V = Span(V)$ for trivial reasons.
\end{proof}

This lemma suggests that spanning sets are not only not unique, they can have vastly different sizes. For example, $\mathbb R^2$ is spanned by $\{{\bf e}_1, {\bf e}_2\}$. It is also spanned by the set $\mathbb R^2$ itself, which is much larger. It is natural to ask how small a spanning set can be. This leads to

\begin{definition} $S$ is a {\it minimal spanning set} for $V$ if
\begin{itemize} 
\item $V = Span(S)$, and
\item For any proper subset $T\subsetneq S$, $Span(T)\subsetneq V$.
\end{itemize}
\end{definition}
\vskip.2in

Recall that for a matrix $A$ we write $A(i,:)$ for the ith row, and $A(:,j)$ for the jth column. If $A$ is an $m\times n$ matrix, then each row $A(i,:)$ is a row vector in the vector space $\mathbb R^{1\times n}$, while each column is a vector in the vector space $\mathbb R^{m\times 1} = \mathbb R^m$. This leads to the following definition

\begin{definition}If $A$ is an $m\times n$ matrix the \underbar{row space of $A$} is the subspace $R(A) := Span\{A(i,:)\}_{1\le i\le m}\subset\mathbb R^{1\times n}$, while the \underbar{column space of A} is the subspace $C(A) := Span\{A(:,j)\}_{1\le i\le n}\subset\mathbb R^{m\times 1}$.
\end{definition}

Note that the row and column spaces of a matrix are different vector spaces, unless $m=n=1$. If $m=n > 1$ then $\mathbb R^{1\times n}$ and $\mathbb R^{n\times 1}$ can be naturally identified via the transpose operation, but they are not the same space.
\vskip.2in

Finding spanning sets for $R(A)$ or $C(A)$ is not a problem; they are just the set of rows and columns of $A$. However if one wants to find a {\it minimal} spanning set then some work needs to be done. The main tool we will use is the following theorem.

\begin{theorem} If $A$ and $B$ are matrices with $A$ row equivalent to $A$ then $R(A) = R(B)$ (their row spaces are the same). Moreover linear relations among the columns of $A$ and $B$ are the same. This applies in particular if $B = rref(A)$.
\end{theorem}

We also have the following useful lemma, which is relatively straightforward to prove.

\begin{lemma} If $A$ is in reduced row echelon form, then 
\begin{itemize}
\item the columns of $A$ containing leading ones are a linearly independent set of column vectors. Moreover, the columns that don't contain leading ones can be written as linear combination of the ones that do. Consequently, the columns of $A$ which contain leading ones form a minimal spanning set for the column space $C(A)$;
\item the rows of $A$ containing leading ones are a linearly independent set of row vectors. As all remaining rows must be identically zero, the rows of $A$ which contain leading ones form a minimal spanning set for the row space $R(A)$.
\end{itemize}
\end{lemma}

This theorem and lemma provide an efficient algorithm for determining minimal spanning sets, as well as determining linear relations among vectors.
\vskip.2in

Given a matrix $A$, a minimal spanning set for $C(A)$ is found by
\begin{itemize}
\item computing $rref(A)$;
\item identifying the set $D = \{i_1,\dots, i_k\}$ of pivot columns;
\item choosing as your minimal spanning set for ${\cal C}(A)$ to be the subset of column vectors
\[
\{A(:,i_1),\dots, A(:,i_k)\}
\]
\end{itemize}

Moreover, in the reduced row echelon form the columns that are not pivot columns are easily computed as linear combinations of the pivot columns. And since the row space remains unchanged under row operations, a minimal spanning set for $R(A)$ is found by
\begin{itemize}
\item computing $rref(A)$;
\item identifying the set $D = \{1,\dots,k\}$ of indices corresponding to the non-zero rows of $rref(A)$;
\item choosing as your minimal spanning set for $R(A)$ the subset of row vectors
\[
\{A(1,:),\dots, A(k,:)\}
\]
\end{itemize}

Note that, unlike the column case, these rows typically won't be among the original set of rows of $A$; they will simply have the same span.


\begin{example} Suppose $A$ is a $4\times 5$ matrix with 
\[
B = rref(A) = \begin{bmatrix}
1 & 0 & 3 & 0 & 4\\
0 & 1 & -2 & 0 & 5\\
0 & 0 & 0 & 1 & 7\\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
We don't know what the original matrix is, but for $B = rref(A)$ we see that
\begin{itemize}
\item columns 1,2, and 4 of $B$ are linearly independent;
\item $B(:,3) = 3B(:,1) - 2B(:,2)$;
\item $B(:,5) = 4B(:,1) + 5B(:,2) + 7B(:,4)$.
\end{itemize}
So we can conclude the same must be true for $A$: 
\begin{itemize}
\item columns 1,2, and 4 of $A$ are linearly independent;
\item $A(:,3) = 3A(:,1) - 2A(:,2)$;
\item $A(:,5) = 4A(:,1) + 5A(:,2) + 7A(:,4)$.
\end{itemize}
Therefore, $\{A(:,1), A(:,2), A(:,4)\}$ form a minimal spanning set for the columns space $C(A)$, while the rows $B(1,:), B(2,:), B(3,:)$ form a minimal spanning set for the row space $R(B) = R(A)$.
\end{example}

\begin{exercise} Suppose $A$ is a $5\times 6$ matrix with
\[
rref(A) = \begin{bmatrix}
1 & 0 & 6 & 0 & 0 & 7\\
0 & 1 & -11 & 0 & 0 & 9\\
0 & 0 & 0 & 1 & 0 & 11\\
0 & 0 & 0 & 0 & 1 & 13\\
0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
\begin{itemize}
\item Find a minimal spanning set for $C(A)$ from the original set of columns of $A$ (expressed simply in terms of the original set of column vectors);
\item For each remaining column of $A$, write it explicitly as a linear combination of the minimal spanning set you have just constructed.
\item Write down a minimal spanning set for the row space $R(A)$ (here you can give explicit numerical vectors).
\end{itemize}

\end{exercise}



The above Theorem and Lemma have additional implications, which we summarize in the following corollary.


\begin{corollary}If $A$ is an $m\times n$ matrix, then 
\begin{itemize}
\item the columns of $A$ are linearly independent precisely when every {\it column} of $rref(A)$ contains a leading 1 (is a pivot column);
\item the columns of $A$ span $\mathbb R^m$ (that is, $C(A) = \mathbb R^m$) precisely when each {\it row} of $rref(A)$ contains a leading 1 (equivalently, is non-zero).
\end{itemize}
As a consequence, we see
\begin{itemize}
\item if $A$ is $m\times n$ with $m > n$ then the columns may be linearly independent, but they cannot span all of $\mathbb R^m$;
\item if $A$ is $m\times n$ with $m < n$ then the columns may span all of $\mathbb R^m$ but cannot be linearly independent;
\item if $A$ is $m\times n$ with $m = n$ then $C(A) = \mathbb R^m$ if and only if the columns of $A$ are linearly independent, if and only if the columns of $A$ are a basis for $\mathbb R^m$. Moreover this happens precisely when $rref(A) = Id$.
\end{itemize}
\end{corollary}


\end{document}
