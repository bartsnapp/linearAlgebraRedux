\documentclass{ximera}
\input{../preamble.tex}
\title{Change of basis}
\author{Crichton Ogle}

\begin{document}
\begin{abstract}
  Determine how the matrix representation depends on a choice of basis.
\end{abstract}
\maketitle

Suppose now that $V$ is an $n$-dimensional vector space equipped with two bases $S_1 = \{{\bf v}_1,{\bf v}_2,\dots,{\bf v}_n\}$ and $S_2 = \{{\bf w}_1, {\bf w}_2,\dots,{\bf w}_n\}$ (we are assuming here the fact, listed above, that any two bases for $V$ must have the same number of elements). Taking $L=Id$, Theorem \ref{thm:matrep} yields the equation
\begin{equation}\label{eqn:basechange}
{}_{S_2}({\bf v}) = {}_{S_2}(Id*{\bf v}) = {}_{S_2}Id_{S_1}*{}_{S_1}{\bf v}
\end{equation}
where 
\begin{equation}\label{eqn:basechangematrix}
{}_{S_2}Id_{S_1} = [{}_{S_2}{\bf v}_1\ {}_{S_2}{\bf v}_2\ \dots\ {}_{S_2}{\bf v}_n]
\end{equation}
The matrix ${}_{S_2}Id_{S_1}$ is referred to as a {\it base transition matrix}, and written as ${}_{S_2}T_{S_1}$. In words, equations (\ref{eqn:basechange}) and (\ref{eqn:basechangematrix}) tells us that {\it in order to compute the coordinate vector ${}_{S_2}{\bf v}$ from ${}_{S_1}{\bf v}$, we multiply ${}_{S_1}{\bf v}$ on the left by the $n\times n$ matrix whose $i^{th}$ column is the coordinate vector of ${\bf v}_i$ with respect to the basis $S_2$}.
\vskip.2in

\begin{theorem} Suppose $S_i,1\le i\le 3$ are three bases for $V$. Then one has the following equalities
\begin{itemize}
\item ${}_{S_3}T_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}$
\item ${}_{S_i}T_{S_i} = Id$
\item ${}_{S_i}T_{S_j} = \left({}_{S_j}T_{S_i}\right)^{-1}$
\end{itemize}
\end{theorem}

\begin{exercise} Verify these three properties (notice that the second and third properties are closely related, in light of the first. Note also that the third property verifies that base transition matrices are always non-singular).
\end{exercise}

For vector spaces other than $\mathbb R^n$ (such as the function space $F[a,b]$ we looked at earlier) where the vectors do not naturally look like column vectors, we always use the above notation when working with their coordinate representations.
\vskip.1in
However, in the case of $\mathbb R^n$, the vectors were {\it defined} as column vectors even before discussing coordinate representations. So what should we do here?
\vskip.1in
The answer is that the vectors in $\mathbb R^n$ are, by convention, identified with their coordinate representations in the {\it standard basis} ${\bf e} = \{{\bf e}_1,\dots,{\bf e}_n\}$ for $\mathbb R^n$. So, for example, in $\mathbb R^3$ when we wrote ${\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}$ what we really meant was that $\bf v$ is the vector in $\mathbb R^3$ with coordinate representation in the standard basis given by ${}_{\bf e}{\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}$.
\vskip.1in
Because it is very important to keep track of bases whenever determining base transition matrices and computing new coordinate representations, when doing so we will {\it always} use base-subscript notation when working with coordinate vectors, even when the vectors are in $\mathbb R^n$ and are being represented in the standard basis.

\begin{example} Suppose $S = \{{\bf u}_1, {\bf u}_2, {\bf u}_3\}$ are three vectors in $\mathbb R^3$ with
\[
{}_{\bf e}{\bf u}_1 = \begin{bmatrix} 1\\0\\2\end{bmatrix},\quad
{}_{\bf e}{\bf u}_2 = \begin{bmatrix} -2\\3\\1\end{bmatrix},\quad
{}_{\bf e}{\bf u}_3 = \begin{bmatrix} 0\\1\\-1\end{bmatrix}
\]
Suppose we want to
\begin{itemize}
\item Show that the set of vectors $S$ is a basis for $\mathbb R^3$,
\item compute the base transition matrix ${}_{S}T_{\bf e}$,
\item for $\bf v$ in $\mathbb R^3$ with ${}_{\bf e}{\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}$, compute the coordinate representation of $\bf v$ with repsect to the basis $S$.
\end{itemize}
To perform step 1, since $S$ has the right number of vectors to be a basis for $\mathbb R^3$, it suffices to show the vectors are linearly independent. And we know how to do this; we form the matrix $A = [{}_{\bf e}{\bf u}_1\ {}_{\bf e}{\bf u}_2\ {}_{\bf e}{\bf u}_3]$ and show that the columns are linearly independent by showing $rref(A) = Id^{3\times 3}$ (exercise: do this, using MATLAB or Octave). This verifies $S$ is a basis.
\vskip.1in
Next, we look at the matrix $A$. The columns of $A$ are the coordinate representations of the vectors in $S$ with respect to the standard basis $\bf e$. But $S$ is a basis. So the matrix $A$ identifies as a base-transition matrix. We know it must be either ${}_S T_{\bf e}$ or ${}_{\bf e}T_S$. But which one?
\vskip.1in
This is where the notation being used helps us. The coordinate vectors of the columns of $A$ have ``$\bf e$" in the lower left. The rule is that this must match what appears in the notation of the transition matrix. So:
\[
{}_{\bf e}T_S = A
\]
To complete the second step, we then compute
\[
{}_S T_{\bf e} = \left({}_{\bf e} T_S\right)^{-1} 
\]
Finally, we can use this to compute ${}_S {\bf v}$ as
\[
{}_S {\bf v} = {}_S T_{\bf e}* {}_{\bf e}{\bf v}
\]
\end{example}
\vskip.2in

\begin{exercise} Let $S_1,S_2$ be two bases for $V$, and $L:V\to V$ a linear transformation from $V$ to itself. We can consider The representations ${}_{S_1}L_{S_1}$ and ${}_{S_2}L_{S_2}$ of $L$ with respect to the bases $S_1$ and $S_2$. Using the above identities, show that
\[
{}_{S_1}L_{S_1} = A*{}_{S_2}L_{S_2}*A^{-1}
\]
where $A = {}_{S_1}T_{S_2}$.
\end{exercise}

Note: Square matrices $B,C$ which satisfy the equality $B = A*C*A^{-1}$ are called {\it similar}. This is an important relation between square matrices, and plays a prominent role in the theory of eigenvalues and eigenvectors as we will see later on.
\vskip.5in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
