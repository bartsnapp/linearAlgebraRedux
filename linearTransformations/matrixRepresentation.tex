\documentclass{ximera}
\input{../preamble.tex}
\title{Matrix representations of transformations}
\author{Crichton Ogle}

\begin{document}
\begin{abstract}
  A linear transformation can be represented in terms of multiplication by a matrix.
\end{abstract}
\maketitle

Suppose $V = \mathbb R^n, W = \mathbb R^m$, and $L_A:V\to W$ is given by 
\[
L_A({\bf v}) = A*{\bf v}
\]
for some $m\times n$ real matrix $A$. Then it follows immediately from the properties of matrix algebra that $L_A$ is a linear transformation:
\[
L_A(\alpha{\bf v} + \beta{\bf w}) 
= A*(\alpha{\bf v} + \beta{\bf w})
= \alpha(A*{\bf v}) + \beta(A*{\bf w})
= \alpha L_A({\bf v}) + \beta L_A({\bf w})
\]
\vskip.2in

Conversely, suppose the linear transformation $L$ is given. Define the matrix $A_L$ by
\[
A_L = [L({\bf e}_1)\ L({\bf e}_2)\ \dots L({\bf e}_n)]
\]
that is, the $m\times n$ matrix with $A(:,i) = L({\bf e}_i),\, 1\le i\le n$. Then by construction
\[
A_L*({\bf e}_i) = A(:,i) = L({\bf e}_i),\, 1\le i\le n
\]
so that ${\bf v}\mapsto L({\bf v})$ and ${\bf v}\mapsto A_L*{\bf v}$ are two linear transformations which agree on a basis for $\mathbb R^n$, which by the previous corollary implies
\[
L({\bf v}) = A_L*({\bf v})\qquad \forall{\bf v}\in \mathbb R^n
\]
Because of this, the matrix $A_L$ is referred to as a {\it matrix representation} of $L$. Note that this representation is with respect to to the standard basis for $\mathbb R^n$ and $\mathbb R^m$.
\vskip.2in

We see now that the same type of representation applies for arbitrary vector spaces {\it once a basis has been fixed for both the domain and target}. In other words, given
\begin{itemize}
\item A vector space $V$ with basis $S = \{{\bf v}_1,\dots,{\bf v}_n\}$,
\item a vector space $W$ with basis $T = \{{\bf w}_1,\dots,{\bf w}_m\}$, and
\item a linear transformation $L:V\to W$
\end{itemize} 
we could ask if there is a similar representation of $L$ in terms of a matrix (which depends on these two choices of bases). The answer is ``yes".

\begin{theorem}\label{thm:matrep} For any ${\bf v}\in V$
\[
{}_TL({\bf v}) = {}_TL_S*{}_S{\bf v}
\]
where ${}_TL_S$ is the $m\times n$ matrix defined  by
\[
{}_TL_S = [{}_TL({\bf v}_1)\ {}_TL({\bf v}_2)\ \dots\ {}_TL({\bf v}_n)]
\]
\end{theorem}

\begin{proof} Again by the above corollary it suffices to verify the equality for basis vectors. But ${}_S{\bf v}_i$ is the $n\times 1$ coordinate vector identical to the basis vector ${\bf e}_i$ for $\mathbb R^n$. From this we get
\[
{}_TL({\bf v}_i) = {}_TL_S(:,i) = {}_TL_S*{}_S{\bf v}_i,\qquad 1\le i\le n
\]
completing the proof.
\end{proof}

\begin{example} Suppose $L:\mathbb R^3\to \mathbb R^2$ is the linear transformation given by
\[
L({\bf x}) = L\left(\bmatrix x_1\\x_2\\x_3\endbmatrix\right) = \bmatrix 2x_1 - 3x_2 + 7x_3\\ 4x_2 - 5x_3\endbmatrix
\]
To compute the matrix representation of $L$ with respect to the standard bases of $\mathbb R^3$ and $\mathbb R^2$, we evaluate $L$ on the basis vectors ${\bf e}_i, 1\le i\le 3$ and then form the matrix $A_L = \bmatrix {L(\bf e}_1) & L({\bf e}_2) & L({\bf e}_3)\endbmatrix$, yielding
\[
A_L = \bmatrix 2 & -3 & 7\\ 0 & 4 & -5 \endbmatrix
\]
\end{example}

\begin{example} Recall that $P_n$ is the vector space of polynomials in one variable with real coefficients. Let $L:P_3\to P_3$ be given by $L(p(x)) = 2p'(x) - p(x)$, where $p'(x)$ denotes the derivative of $p(x)$. To find the matrix representation of $L$ with respect to the basis $\{1,x,x^2\}$ for $P_3$, we first compute $L(1) = -1; L(x) = 2 - 2x; L(x^2) = 4x - x^2$. Then
\[
A_L = \bmatrix L(1) & L(x) & L(x^2)\endbmatrix = \bmatrix -1 & 2 & 0\\0 & -2 & 4\\0 & 0 & -1\endbmatrix
\]
\end{example}

Returning once more to the general case where $L:V\to W$ is linear, $S$ a basis for $V$, $T$ a basis for $W$, we note that the bases $S$ and $T$ can be used to identify the kernel and image of $L$. Precisely, we have

\begin{theorem} Assume $Dim(V) = n$, $Dim(W) = m$. Let $A = {}_{T}L_{S}$ be the $m\times n$ matrix representation of $L$ with respect to the pair of bases $S,T$. Then
\begin{itemize}
\item ${\bf v}\in ker(L)\subset V$ iff ${}_{S}{\bf v}\in N(A)\subset\mathbb R^n$.
\item ${\bf w}\in im(L)\subset W$ iff ${}_{T}{\bf w}\in C(A)\subset\mathbb R^m$.
\end{itemize}
\end{theorem}

This theorem tells us that, once we have fixed a basis for $V$ and $W$, the representation of $L$ by the matrix $A={}_{T}L_{S}$ further identifies i) the kernel $ker(L)$ of $L$ with the nullspace $N(A)$ of $A$ and ii) the image $im(L)$ with the column space $C(A)$ of $A$.
\vskip.2in

\begin{example} Suppose $L: P_3\to P_3$ is the linear transformation represented with respect to the standard basis on $P_3$ by the matrix $ A = \bmatrix 2 & 3 & 1\\ 3 & 9 & 6\\ 1 & 6 & 5\endbmatrix$. Our objective is to find a minimal spanning set $ker(L)$ and $im(L)$ (with respect to the standard basis for $P^3$).
\vskip.1in
First we compute $rref(A) = \bmatrix 1 & 0 & -1\\0 & 1 & 1\\0 & 0 & 0\endbmatrix$. We then see
\begin{itemize}
\item The column space $C(A)$ of $A$ is $Span\{A(:,1), A(:,2)\} = Span\left\{\bmatrix 2\\3\\\1\endbmatrix, \bmatrix 3\\9\\6\endbmatrix\right\}$;
\item  the nullspace $N(A)$ of $A$ is $Span\left\{\bmatrix 1\\-1\\1 \endbmatrix \right\}$. Note that these are coordinate vectors.
\item The corresponding vectors, written as polynomials in $P_3$, give
\[
im(L) = Span\left\{(2 + 3x + x^2), (3 + 9x + 6x^2)\right\},\qqua ker(L) = Span\left\{(1 - x + x^2)\right\}
\]
\end{itemize} 
\end{example}

\begin{exercise} Suppose $L:P_4\to P_4$ is the linear transformation whose representation in the standard basis for $P_4$ is given by
\[
A =\bmatrix 10 & -16 & -2 & -2\\
  -16 & 36 & 22 & 12\\
   -2 & 22 & 53 & -4\\
   -2 & 12 & -4 & 30\endbmatrix
\]
Following the method in the above example, write down a minimal spanning set for $ker(L)$ and $im(L)$ (the elements should be vectors in $P_4$ - i.e., polynomials, not their coordinate representations.)
\end{exercise}


\vskip.3in

\end{document}
