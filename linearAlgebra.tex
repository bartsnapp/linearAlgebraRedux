\documentclass[11pt,notitlepage]{article}
\usepackage{amsmath, amsthm, amssymb,todonotes, enumerate,color,pigpen,mathtools}
\usepackage{graphicx}%authblk
\usepackage[v2]{xy}
\xyoption{all}
\topmargin  = -1cm
\textwidth  = 16cm \textheight = 24cm
\voffset-.75cm\hoffset=-1.7cm
\parindent  = 0pt
\pdfpageheight 11in
\pagestyle{empty}
%\flushbottom
%\renewcommand\Authfont{\scshape}
%\renewcommand\Affilfont{\small}

\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}[equation]{Theorem}
\newtheorem*{thm}{Theorem}
\newtheorem{proposition}[equation]{Proposition}
\newtheorem{lemma}[equation]{Lemma}
\newtheorem{corollary}[equation]{Corollary}
\newtheorem{conjecture}[equation]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[equation]{Definition}
\newtheorem{example}[equation]{Example}
\newtheorem{exercise}[equation]{Exercise}
\newtheorem{remark}[equation]{Remark}
\newtheorem{observation}[equation]{Observation}
\newtheorem{question}[equation]{Question}
\newtheorem*{ques}{Question}
\newtheorem{construction}[equation]{Construction}
\newtheorem{claim}[equation]{Claim}


\newcommand\wt{\widetilde}
\newcommand\ov{\overline}
\newcommand\inj{\rightarrowtail}
\newcommand\surj{\twoheadrightarrow}
\newcommand{\harpoon}{\overset{\rightharpoonup}}

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\begin{document}


\title{Linear Algebra}
\author{}
\maketitle
\vskip.3in

\tableofcontents

\section{Preface} This represents the first draft of the OSU Department of Mathematics' online Linear Algebra course. The sections below includes everything covered in the standard Math 2568 course, and a bit more. Later drafts will incorporate additional topics from applied linear algebra and differential equations.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Systems of equations} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definition} A {\it linear function in one variable} is of the form $f(x) = ax+b$ where $x$ is a variable and $a,b$ are numbers (or scalars, as they are referred to in linear algebra). It is {\it homogeneous} if $b=0$. Similarly, a {\it linear function in n variables} is one of the form
\[
f(x_1,x_2,\dots,x_n) = a_1x_1 + a_2x_2 + \dots a_nx_n + b
\]
where the $x_i$ are variables (or unknowns) and the $a_i$ are scalars. Again, the linear function is called {\it homogeneous} if the constant term $b$ is zero. Next, a {\it linear equation in n variables} is one of the form
\[
a_1x_1 + a_2x_2 + \dots a_nx_n =  b
\]
where the expression on the left is a linear homogeneous function in $n$ variables, and the term on the right is a constant. So, for example, 
\begin{equation}\label{eqn:ex1}
3x_1 - 7x_2 + 11x_3 = 14
\end{equation}

is a linear equation in three variables - $x_1, x_2$, and $x_3$. A {\it solution} to a linear equation is an assignment of values to each of the variables appearing which makes the equation hold true. Returning to the example in the equation (\ref{eqn:ex1}), we see
\[
x_1 = 1, x_2 = 0, x_3 = 1
\]
is a solution, because when substituted into the left-hand side it results in the value 14. Finally, a {\it system of equations} is a collection of linear equations in the same set of variables, or unknowns
\begin{alignat*}{5}\tag{2.2}\label{eqn:sys}
a_{11}x_1 &&+ a_{12}x_2 && + {}\ldots{} && + a_{1n}x_n && =  b_1 &\\
a_{21}x_1 + && a_{22}x_2 + &&  {}\ldots{} && + a_{2n}x_n &&  = b_2 &\\
\vdotswithin{a_{m1}x_1} &&  \vdotswithin{a_{m2}x_2} &&  {}\ldots{} &&  \vdotswithin{a_{mn}x_n} &&   \vdotswithin{b_m} &\\
a_{m1}x_1 &&+ a_{m2}x_2 && + {}\ldots{} && + a_{mn}x_n && =  b_m &
\end{alignat*}

and a {\it solution} to that system is an assignment of values to the variables which make {\it each equation} hold true. Note that systems of equations, or even a single equation, need to have a solution. To illustrate, consider the equation
\[
0x_1 + 0x_2 + 0x_3 = 4
\]
Obviously, any set of values substituted into the left-hand side of this equation will produce the value zero, which is not equal to 4. But even non-zero systems might not have a solution. As an example, consider
\begin{alignat*}{3}
x_1 && - 2x_2 && =& \phantom{1}7 \\
2x_1 && - 4x_2 && =& 16 
\end{alignat*}
The left-hand side of the second equation is twice that of the first. So if we take any solution of the first equation and plug in those values on the left-hand side of the second equation, we will always get $2*7 = 14\ne 16$. Therefore, these two equations will never be simultaneously satisfied, and so the system doesn't have a solution.
\vskip,2in
A system which has at least one solution is called {\it consistent}. If it doesn't have any solutions it is called {\it inconsistent}. The main questions we need to answer are:
\vskip.1in

\begin{question} Given a system of equations, does it have one or more solutions (in other words is it consistent)?
\end{question}
\vskip.05in
\begin{question} If it is consistent, what are the solutions?
\end{question}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Finding solutions} To begin with, some terminology. A system consisting of $m$ equations in the same collection of $n$ unknowns is referred to as an $m\times n$ system, which reads as: \lq\lq m-by-n system\rq\rq; such a system is illustrated in (2.2). The number of rows $m$ and the number of columns $n$ are called the {\it dimensions} of the system. The system is
\begin{itemize}
\item {\it Underdetermined} if $m < n$ (less equations than unknowns);
\item {\it Overdetermined} if $m > n$ (more equations than unknowns);
\item {\it Balanced} if $m = n$ (same number of equations as unknowns).
\end{itemize}
Also,
\begin{itemize}
\item The {\it solution set} of a system of equations is the collection (or set) of all solutions;
\item Two $m\times n$ systems are {equivalent} if and only if they have the same solution set.
\end{itemize}

Given a system of equations, how can we find its solution set? The answer is by finding a simpler but equivalent system for which the solution set is easily read off. The method used for finding this simpler system is called {\it row reduction}, and the operations used to perform row reduction are called {\it row operations}, of which there are three types. 
\vskip.1in
\begin{enumerate}
\item[[{\bf Type I}]] Switching two rows;
\item[[\bf{Type II}]] Multiplying a row by a non-zero scalar $\alpha$;
\item [[\bf{Type III}]]Adding a multiple of one row to another distinct row.
\end{enumerate} 
\vskip.1in
It is quite easy to see the the first two types of row operations won't change the solution set, and it can also be shown that the third doesn't either. We'll say that two systems of the same dimensions are {\it row equivalent} if one can be gotten from the other by a sequence of row operations. The following theorem tells us about when this happens.

\begin{theorem} Two systems of the same dimensions are row equivalent if and only if they are equivalent.
\end{theorem}

Now to proceed in an efficient manner, it will be convenient to represent a system in terms of its essential information. This is accomplished by means of the {\it augmented coefficient matrix} or {\it ACM} corresponding to a given system. Starting with the $m\times n$ system in (2.2), the augmented coefficient matrix is given by
\[
A = 
\begin{amatrix}{4}
a_{11} &a_{12} &{}\ldots{} &a_{1n} &b_1 \\
a_{21} &a_{22} &{}\ldots{} &a_{2n} &b_2 \\
\vdotswithin{a_{m1}} &\vdotswithin{a_{m2}} &{}\ldots{} &\vdotswithin{a_{mn}} &\vdotswithin{b_m} \\
a_{m1} &a_{m2} &{}\ldots{} &a_{mn} &b_m 
\end{amatrix}
\]
This is a matrix with $m$ rows and $(n+1)$ columns. Executing a number of row operations and then computing the ACM gives the same result as first forming the ACM and performing the row operations on that matrix. Because the ACM contains all of the information relevant for solving the system, and is less cumbersome to work with, we will always form the ACM {\it first}, and perform  row operations on that matrix. 
\vskip.2in

The simplified form in which we would like to get our matrix is referred to as {\it reduced row echelon form}. This is a term which applies to matrices in general, not just augmented coefficient matrices.

\begin{definition} A matrix $B$ of numbers is in {\it row-echelon form} if
\begin{itemize}
\item Every row of zeros lies below every non-zero row;
\item the left-most non-zero entry of a non-zero row is 1 (called a {\it leading} 1);
\item if both row $k$ and row $(k+1)$ are non-zero, the leading 1 in row $(k+1)$ appears to the right of the leading row in row $k$.
\end{itemize}
It is in {\it reduced row-echelon form} if in addition to being in row echelon form it satisfies the property
\begin{itemize}
\item In every column that contains a leading 1, that leading 1 is the only non-zero entry.
\end{itemize}
\end{definition}

\begin{example} Consider the three matrices
\[
A = \begin{bmatrix}
1 & 2 & -1 & 4\\
0 & 0 & 1 & 3\\
0 & 1 & 0 & 2
\end{bmatrix},\qquad
B = \begin{bmatrix}
1 & 2 & 3 & 4\\
0 & 1 & 5 & 7\\
0 & 0 & 0 & 1
\end{bmatrix},\qquad
C = \begin{bmatrix}
1 & 0 & 0 & 4\\
0 & 1 & 0 & 3\\
0 & 0 & 1 & 1
\end{bmatrix}
\]
\end{example}

The matrix $A$ is not in row echelon form, while $B$ is in row echelon but not reduced row echelon form, and $C$ is in reduced row echelon form (the reader should check this, and understand why for each example). The main fact we will need to know is

\begin{theorem} Every matrix of numbers is row-equivalent to one which is in reduced row echelon form.
\end{theorem}
\vskip.2in

The reduced row echelon form of a matrix is unique; for that reason we will refer to {\it the} reduced row echelon form of a matrix $A$, and write it as $rref(A)$. When $A$ is the ACM of a system of equations, $rref(A)$ tells us essentially everything we would like to know about the original system. The way it does this is summarized by the next result.

\begin{theorem} Let $A$ be the ACM of an $m\times n$ system of equations. Then the system
\begin{itemize}
\item is inconsistent precisely when $rref(A)$ contains a row of the form
\[
[0\ \ 0\ \ 0\ \ 0\ \dots\ \ 0\ \ |\ \ 1];
\]
\item has a unique solution precisely when each column of $rref(A)$ except the right-most column contains a leading 1;
\item has infinitely many solutions when it is i) consistent, and ii) at least one of the first n columns of $rref(A)$ does not contain a leading 1.
\end{itemize}
In the last case, the solution set is parametrized by the variables appearing in the original system which are indexed by the columns of $rref(A)$ to the left of the bar which do not contain a leading 1.
\end{theorem}

It is important to note that when $A$ is the ACM of a system, $rref(A)$ is the ACM of another system equivalent to the original one, which is the reduced row echelon form of the original system. In this reduced row echelon form, it {\it is} possible for an equation to consist of all zeros. If it does, we do not delete it from the system, because we want to maintain the original dimensions.

\begin{example} The rref of the ACM is given by
\[
rref(A) = 
\begin{amatrix}{4}
1 & 2 & 3 & 0 & 5\\
0 & 0 & 0 & 1 & 7 \\
0 & 0 & 0 & 0 & 1
\end{amatrix}
\]
In this case, we would conclude that the system is {\it inconsistent}, because of the last row (to understand why a row like this makes the system inconsistent, the reader should write down the equation to which it corresponds, and see what they can say about solutions to that single equation).
\end{example}
\vskip.2in
\begin{example} The rref of the ACM is given by
\[
rref(A) = 
\begin{amatrix}{4}
1 & 0 & 0 & 0 & 5\\
0 & 1 & 0 & 0 & -2 \\
0 & 0 & 1 & 0 & 4\\
0 & 0 & 0 & 1 & 10
\end{amatrix}
\]

In this case, every column to the left of the bar dividing the {\it coefficient matrix} from the augmented part contains a leading 1. Hence there is a unique solution (the reader should write down the equations corresponding to this ACM, and see how those equations actually give the exact solution).
\end{example} 
\vskip.2in

\begin{example} The rref of the ACM is given by
\[
rref(A) = 
\begin{amatrix}{4}
1 & 0 & 0 & 3 & 7\\
0 & 1 & 0 & 1 & 5 \\
0 & 0 & 1 & 2 & 2\\
0 & 0 & 0 & 0 & 0
\end{amatrix}
\]
Here we have a $4\times 4$ system which has infinitely many solutions; this is seen by noting i) it is consistent, and ii) the fourth column does not contains a leading 1 (the reader should write down the four equations corresponding to this matrix and then rewrite each equation so that they express the original four variables $x_1,x_2,x_3,x_4$ as linear functions in $x_4$, which is the independent parameter in this case. Since we only have one parameter, the result is a {\it one-parameter family of solutions}).
\end{example}
\vskip.2in

In the process of putting the ACM in reduced row echelon form, it is often desirable to keep track of the precise row operations being used. For this, some notation is useful. The following table summarizes the notation we have used in class.
\vskip.2in

\begin{center}
    \begin{tabular}{|c|c|c|}\hline\hline
    \phantom{x} & \phantom{x} & \phantom{x}\\
    \Large{Type} &\Large{What it does} &\Large{Indicated by}\\ \hline
    \phantom{x} & {\large Switches} & \phantom{\Huge X}\\
    \large{Type I} &{\large $i^{th}$ and $j^{th}$} & {\large $R_i\leftrightarrow R_j$}\\ 
    \phantom{x} & {\large rows} & \phantom{x}\\ \hline
    \phantom{x} &{\large Multiplies} & \phantom{\Huge X}\\
    \large{Type II} &{\large $i^{th}$ row} & {\large $r\cdot R_i$}\\
    \phantom{x} & {\large by $r\ne 0$} & \phantom{x}\\ \hline
    \phantom{x} &{\large Adds} & \phantom{\Huge X}\\
    \large{Type III} &{\large $a$ times the $i^{th}$ row} & {\large $a\cdot R_i$ added to $R_j$}\\
    \phantom{x} & {\large to the $j^{th}$ row} & \phantom{x}\\\hline\hline
    \end{tabular}
    \end{center}
\vskip.2in

At this point, we see that the reduced row echelon form of the ACM allows us to solve the system. However, we have not discussed how the transition to that form is accomplished. The following algorithm describes that process (this appears as steps 1 - 6 on p. 20 of the text).
\vskip.2in
\begin{enumerate}
\item[{\bf Step 1}] Determine the left-most column containing a non-zero entry (it exists if the matrix is non-zero).
\item[{\bf Step 2}] If needed, perform a type I operation so that the first non-zero column has a non-zero entry in the first row.
\item[{\bf Step 3}] If needed, perform a type II operation to make that first non-zero entry 1 (the leading 1 in the first row).
\item[{\bf Step 4}] Perform type III operations to make the entries below this leading 1 equal to 0.
\item[{\bf Step 5}] Repeat the previous four steps on the submatrix consisting of all except the first row, until reaching the end of the rows.
\item[{\bf Step 6}] For each row containing a leading 1, proceed upward using type III operations to  make zero any entry appearing above a leading 1.
\end{enumerate}
\vskip.2in
To summarize,

\begin{theorem} Every system of equations is uniquely represented by its associated augmented coefficient matrix (ACM), and every ACM results from a unique system of equations, up to a labeling of the indeterminates. The solution set to the system can be determined by i) putting the ACM in reduced row echelon form (rref), and ii) reading off the solution(s) from the resulting matrix. Moreover, the computation of rref(ACM) can be performed in a systematic fashion, by following the algorithmic procedure listed above.
\end{theorem}

\vskip.5in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matrices}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Matrix Operations (algebraic and otherwise)} We begin by recalling some notation and terminology regarding matrices. A {\it matrix} will mean a rectangular array whose entries are of the same type. Thus, we could have an array of real numbers, complex numbers, functions, or even matrices. An {\it $m\times n$ matrix} will refer to one which has $m$ rows and $n$ columns, and the {\it collection of all $m\times n$ matrices of real numbers} will be denoted by $\mathbb R^{m\times n}$. We adopt the convention, used by MATLAB, in which the $(i,j)^{th}\ entry$ of the matrix $A$ (that in row $i$ and column $j$) is denoted by $A(i,j)$. Also, following MATLAB notation, we will write the $i^{th}$ row as $A(i,:)$, and the $j^{th}$ column as $A(:,j)$. Before getting to the operations themselves, we first record

\begin{definition} Two matrices $A$ and $B$ are {\it equal} if $A(i,j) = B(i,j)$ for all $i,j$.
\end{definition}
Note that this equality forces $A$ and $B$ to have the same dimensions, because if they had different dimensions, there would have to be a choice of indices $(i,j)$ for which one side of the equation exists, but the other does not. Thus, equality can be reformulated as saying: $A$ and $B$ have the same dimensions, and the same entry in each place.
\vskip.2in
Matrix algebra uses three different types of operations.
\vskip.1in

{\bf\underbar{Matrix Addition}} If $A$ and $B$ have the same dimensions, then the sum $A+B$ is given by\footnote{ In what follows, the symbol \lq\lq$:=$\rq\rq\ is used to indicate that the expression to the left of the symbol is {\it defined to be} what appears to the right of the symbol.}
\[
(A+B)(i,j) := A(i,j) + B(i,j)
\]

Note that the dimensions of $A+B$ are the same as those of (both) $A$ and $B$.
\vskip.1in

{\bf\underbar{Scalar Multiplication}} If $A$ is a matrix and $\alpha$ a scalar, the {scalar product} of $\alpha$ with $A$ is given by
\[
(\alpha A)(i,j) := (\alpha)A(i,j)
\]
There are no restrictions on the dimension of $A$ for this operation to be defined.
\vskip.1in

{\bf\underbar{Matrix Multiplication}} This is the most complicated of the three operations. If $A$ is $m\times n$ and $B$ is $p\times q$, then in order for the product $A*B$ to be defined, we require that $n = p$; this condition is expressed by saying that {\it the internal dimensions agree}. In this case
\[
(A*B)(i,k) := \sum_{j=1}^n A(i,j)B(j,k)
\]
The dimensions of the product are $m\times q$.
\vskip.1in

These different types of products may both be viewed as extensions of ordinary multiplication of real numbers. In fact, if we identify numerical $1\times 1$ matrices with scalars, then in that dimension the two product operations both correspond to ordinary multiplication. It is important to note that matrix multiplication can be performed whenever the sum of products appearing on the right-hand side is well-defined; in other words, for more than just numerical matrices. This fact will come into play later on.
\vskip.2in

The operations for matrix algebra satisfy similar properties to those for addition and multiplication of real numbers. The following theorem lists those properties. In each case, the expression on the left is defined iff that on the right is also defined.

\begin{theorem}\label{thm:matalg} Let $A,B,C$ denote matrices, and $\alpha,\beta$ scalars.

\begin{enumerate}
\item $A+B = B+A$ (commutativity of addition);
\item $A+(B+C) = (A+B)+C$ (associativity of addition);
\item $\alpha(A+B) = \alpha A + \alpha B$ (scalar multiplication distributes over matrix addition);
\item $A*(B+C) = A*B + A*C$ (matrix multiplication left-distributes over matrix addition);
\item $(A+B)*C = A*C + B*C$ (matrix multiplication right-distributes over matrix addition);
\item $(\alpha\beta)A = \alpha(\beta A)$ (associativity of scalar multiplication);
\item $A*(B*C) = (A*B)*C$ (associativity of matrix multiplication).
\end{enumerate}
\end{theorem}

This theorem is proven by showing that, in each case, the matrix on the left has the same $(i,j)^{th}$ entry as the one on the right. However, as with any proof, one needs to be clear from the beginning exactly what one is allowed to {\it assume} as being true. In this case, we have i) the definition of what it means for two matrices to be equal, ii) the explicit definition of each operation, and iii) the corresponding properties for addition and multiplication for real numbers (which will be taken as axioms for this proof). To see how this works, let's verify the first equality. 

\begin{proof} (of 3.2.1)
\begin{align*}
(A+B)(i,j) &= A(i,j) + B(i,j)\qquad{\rm by\ the\ definition\ of\ matrix\ addition}\\
                &= B(i,j) + A(i,j)\qquad{\rm by\ commutativity\ of\ addition\ for\ real\ numbers}\\
               &= (B+A)(i,j)\qquad\quad\ {\rm by\ the\ definition\ of\ matrix\ addition}
\end{align*}
\end{proof}

Notice that the proof consists of a sequence of equalities, beginning with the left-hand side of the equation we wish to verify, and ending with the right-hand side of that equation. Moreover, each equality in the sequence is justified by either a definition, or an axiom. Not all of the equalities are that easy; some may require more steps. To illustrate a more involved proof, we will verify property 7 (probably the most difficult to prove of the properties listed).

\begin{proof} (of 3.2.7) In this proof, $i,j,k,l$ will be used as indices (the reason for using four different indices will become apparent).
\begin{alignat*}{3}
(A*(B*C))(i,l)  =& \sum_j A(i,j)(B*C)(j,l)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& \sum_j A(i,j)\left(\sum_k B(j,k)C(k,l)\right)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& \sum_j\left(\sum_k A(i,j)\Big(B(j,k)C(k,l)\Big)\right)&&\qquad{\rm by\ property\ 4\ for\ real\ numbers}&\\
                =& \sum_j\left(\sum_k \Big(A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 7\ for\ real\ numbers}&\\
                =& \sum_k\left(\sum_j \Big(A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 1\ for\ real\ numbers}&\\
                =& \sum_k\left(\Big(\sum_j A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 5\ for\ real\ numbers}&\\
                =& \sum_k\left(\sum_k (A*B)(i,k)C(k,l)\right)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& ((A*B)*C)(i,l)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
\end{alignat*}
\end{proof}
\vskip.2in

\begin{exercise} Using the above two proofs as models, prove properties (3.2.2) - (3.2.6).
\end{exercise}

Before moving on to considering equations, we introduce a few more matrix operations.

\begin{definition} The {\it transpose} of the matrix $A$, written as $A^T$, is always defined, and given by
\[
\left(A^T\right)(i,j) := A(j,i)
\]
\end{definition}
The way this operation relates to the algebraic operations defined above is described by the next theorem.

\begin{theorem} Let $A$ and $B$ be matrices. Then
\begin{enumerate}
\item $(A+B)^T = A^T + B^T$;
\item $(A*B)^T = B^T*A^T$;
\item $\left(A^T\right)^T = A$.
\end{enumerate}
\end{theorem}

\begin{exercise} Verify these properties in the same manner as in the previous exercise.
\end{exercise}

Also, one can concatenate matrices. Specifically,

\begin{definition} If $A$ is $m\times n$ and $B$ is $m\times p$, then the {\it horizontal} concatenation of $A$ and $B$ is written as
\[
\begin{bmatrix} 
A & B
\end{bmatrix};
\] 
it is the $m\times (n+p)$ matrix where $A$ appears as the left-most $m\times n$ block, and $B$ appears as the right-most $m\times p$ block. Similarly, if $C$ is $q\times n$, then the {\it vertical concatenation} of $A$ and $C$ is written as
\[
\begin{bmatrix} 
A\\ C
\end{bmatrix};
\]
It is the $(m+q)\times n$ matrix where $A$ appears in the upper $m\times n$ block, and $C$ in the lower $q\times n$ block.
\end{definition}
Concatenation can be done multiple times. Any matrix $A$ can be viewed as 
\begin{itemize}
\item the horizontal concatenation of its columns, and
\item the vertical concatenation of its rows.
\end{itemize}

In what follows, the horizontal type of concatenation will be used much more often than vertical one; for that reason {\it concatenation} (direction unspecified) will refer to {\it horizontal concatenation}. The following exercise will allow the reader to better understand how concatenation interacts with the algebraic operations, and the transpose. It is not an exhaustive list.

\begin{exercise} Let $A, B, C, D$ denote matrices all with the same number of rows. Show that
\begin{enumerate}
\item If the pairs $A,C$ and $B,D$ the same number of columns as well, then
\[
\begin{bmatrix} A & B\end{bmatrix} + \begin{bmatrix} C & D\end{bmatrix} = \begin{bmatrix} (A+C) & (B+D)\end{bmatrix}.
\]
\item With respect to products, one has
\[
A*\begin{bmatrix} B & C\end{bmatrix} = \begin{bmatrix} A*B & A*C\end{bmatrix}.
\]
\item With respect to transpose, one has
\[
\begin{bmatrix} A & B\end{bmatrix}^T = \begin{bmatrix} A^T \\ B^T\end{bmatrix}
\]
\end{enumerate}
\end{exercise}

Finally, we discuss the identity matrix and inverses. Recall that a {\it number} $\alpha$ is invertible if there is another number $\beta$ such that $\alpha\beta = 1$. A similar notion exists for matrices. To explain it, we first need to define the matrix equivalent of the number \lq\lq 1".

\begin{definition} The {\it identity matrix} $I = I^{n\times n}$ is the $n\times n$ matrix with $I(i,j) = \begin{cases}1\ \ \rm{if}\ i=j\\0\ \ \rm{if}\ i\ne j\end{cases}$.
\end{definition}

In most cases the dimension of $I$ will not be indicated, as it will be uniquely determined by the manner in which it is being used. For example, if it appears as a term in a matrix product, then its dimension is assumed to be the one which makes the product well-defined. This rule applies for the following 

\begin{proposition} For any matrix $A$, $I*A = A$ and $A*I = A$
\end{proposition}

\begin{exercise} Verify these two equalities.
\end{exercise}

\begin{definition} An $m\times n$ matrix $A$ is {\it invertible} if there is an $n\times m$ matrix $B$ satisfying
\[
A*B = I^{m\times m}, B*A = I^{n\times n}
\]
\end{definition}

Apriori, is seems there is no dimensional restriction on a matrix for it to be invertible. However, the following theorem clarifies the situation. The reason for why it is true will become clear later on when we discuss the rank of a matrix.

\begin{theorem} A matrix $A$ can only be invertible if it is square ($m = n$). In this case $A*B =I$ iff $B*A = I$ (every one-sided inverse is a two-sided inverse).
\end{theorem}

\begin{exercise} Show that the inverse of an invertible matrix is unique.
\end{exercise}

Given this, we will refer to {\it the} inverse of an invertible matrix $A$, and write it as $A^{-1}$. An alternative term for invertible is {\it non-singular} (so {\it singular} is equivalent to being  {\it non-invertible}). Two important questions are: under what conditions is a square matrix non-singular? And if a matrix is non-singular, how can one find its inverse? These questions are answered by 

\begin{theorem} Let $A$ be an $n\times n$ matrix. Then either
\begin{itemize}
\item $rref(A) = I$, which happens precisely when $A$ is non-singular;
\item the bottom row of $rref(A)$ is entirely zero, which happens precisely when $A$ is singular.
\end{itemize}
Moreover, when $A$ is non-singular, one has
\[
rref([A\ \ I]) = [I\ \ A^{-1}]
\]
\end{theorem}

\begin{exercise} Using the above theorem, show that the only matrix which is both invertible and in reduced row echelon form is the identity matrix (of a given dimension).

\end{exercise}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Matrix equations}

A matrix with one row is called a {\it row} vector, and if it has one column a {\it column} vector. The term {\it vector}, for now, will refer to a column vector. Matrices and vectors can be used to rewrite systems of equations as a single equation, and there are advantages to doing this. To begin with, notice that the system appearing in (\ref{eqn:sys}) can be expressed as the single {\it vector equation}
\begin{equation}\label{eqn:vec1}
\begin{bmatrix}
a_{11}x_1\ \  + &a_{12}x_2\ \  + &{}\ldots{}\ \  + &a_{1n}x_n\\ 
a_{21}x_1\ \  + &a_{22}x_2\ \  + &{}\ldots{}\ \  + & a_{2n}x_n\\
\vdots\ \  &  \vdots\ \  &  {}\ldots{}\ \  &  \vdots\\
a_{m1}x_1\ \  + &a_{m2}x_2\ \  + &{}\ldots{}\ \  + &a_{mn}x_n 
\end{bmatrix}
=
\begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_m
\end{bmatrix}
\end{equation}
The vector on the left above consists of entries which are linear homogeneous functions in the variables $x_1,x_2,\dots,x_n$. A {\it solution} to this vector equation will be exactly what it was before; and assignment of values to the variables $x_1,x_2,\dots,x_n$ which make the equation true. 
\vskip.2in

Now the expression on the left in (\ref{eqn:vec1}) can be written as a sum of its components, where the $x_i$ component can be derived by setting all of the other variables to zero. The result is 
\begin{equation}\label{eqn:vec2}
\begin{bmatrix}
a_{11}x_1\\ 
a_{21}x_1\\
\vdots\\
a_{m1}x_1
\end{bmatrix} +
\begin{bmatrix}
a_{12}x_2\\ 
a_{22}x_2\\
\vdots\\
a_{m2}x_2
\end{bmatrix} +\dots +
\begin{bmatrix}
a_{1n}x_n\\ 
a_{2n}x_n\\
\vdots\\
a_{mn}x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_m
\end{bmatrix}
\end{equation}
Next we observe that the $i^{th}$ component, which involves only $x_i$, can be factored as
\begin{equation}\label{eqn:veci}
\begin{bmatrix}
a_{1i}x_i\\ 
a_{2i}x_i\\
\vdots\\
a_{mi}x_i
\end{bmatrix}
=
x_i\begin{bmatrix}
a_{1i}\\ 
a_{2i}\\
\vdots\\
a_{mi}
\end{bmatrix}
\end{equation}
Using this, the vector equation (\ref{eqn:vec2}) may be rewritten as
\begin{equation}\label{eqn:vec3}
x_1\begin{bmatrix}
a_{11}\\ 
a_{21}\\
\vdots\\
a_{m1}
\end{bmatrix} +
x_2\begin{bmatrix}
a_{12}\\ 
a_{22}\\
\vdots\\
a_{m2}
\end{bmatrix} +\dots +
x_n\begin{bmatrix}
a_{1n}\\ 
a_{2n}\\
\vdots\\
a_{mn}
\end{bmatrix}
=
\begin{bmatrix}
b_1\\
b_2\\
\vdots\\
b_m
\end{bmatrix}
\end{equation}
The left-hand side of this last equation leads us to one of the central constructions in all of Linear Algebra.

\begin{definition} Given a collection of vectors $\{v_1,v_2,\dots,v_n\}$, a {\it linear combination} of these vectors is a sum of the form
\[
\alpha_1v_1 + \alpha_2v_2 +\dots + \alpha_nv_n
\]
where the coefficients $\alpha_i$ are scalars.
\end{definition}
In words, it is {\it a sum of scalar multiples of the vectors} $v_1,v_2,\dots v_n$. Now the expression on the left of (\ref{eqn:vec3}) is a linear combination of sorts, but where the coefficients are scalar-valued variables rather than actual scalars. So for any assignment of values to the variables $x_1,x_2,\dots x_n$ we get an actual linear combination.
\vskip.2in

Finally, going back to equation (\ref{eqn:vec1}) we observe that the left-hand side can be written as $A*{\bf x}$, where $A$ is the $m\times n$ {\it coefficient matrix}
\begin{equation}
\label{eqn:coeff}
A = \begin{bmatrix}
a_{11}  &a_{12} &{}\ldots{} &a_{1n}\\ 
a_{21} &a_{22} &{}\ldots{} & a_{2n}\\
\vdots\ \  &  \vdots\ \  &  {}\ldots{}\ \  &  \vdots\\
a_{m1} &a_{m2} &{}\ldots{} &a_{mn} 
\end{bmatrix}
\end{equation}
and ${\bf x}$ is the $n\times 1$ vector variable
\begin{equation}
\label{eqn:coeff}
{\bf x} = \begin{bmatrix}
x_1\\ 
x_2\\
\vdots\\
x_n 
\end{bmatrix}
\end{equation}
which leads to our final equivalent form of (\ref{eqn:vec1}), referred to as the {\it matrix equation associated to the system of equations}:
\begin{equation}\label{eqn:mat}
A*{\bf x} = {\bf b}
\end{equation}
where ${\bf b}$ is the vector ${\bf b} := [b_1\  b_2\ \dots\  b_m]^T$. As with (\ref{eqn:vec1}), a solution is an assignment of a particular numerical vector to $\bf x$ making the equation true, and matrix equation is {\it consistent} iff such an {\bf x} exists. Summarizing

\begin{theorem} The system of equations appearing in (\ref{eqn:sys}) is equivalently represented by the vector equations appearing in (\ref{eqn:vec1}), (\ref{eqn:vec2}), (\ref{eqn:vec3}), as well as the matrix equation (\ref{eqn:mat}). Moreover, the system is consistent precisely when the vector {\bf b} can be written as a linear combination of the columns of the coefficient matrix $A$. 
\end{theorem}

\begin{proof} The only point needing verification is the last statement. But this follows from (\ref{eqn:vec3}), which can be more succinctly written as
\[
x_1 A(:,1) + x_2 A(:,2) +\dots x_n A(:,n) = {\bf b}
\]
 since any solution will yield a particular set of values for $x_1,x_2,\dots,x_n$ to take as scalars on the left so that the resulting linear combination produces {\bf b}, while a particular linear combination which results in {\bf b} would in turn produce a solution to (\ref{eqn:vec3}).
\end{proof}

The last part of this theorem is sometimes called the {\it consistency theorem for systems of equations}. We will occasionally refer to it in this way. 
\vskip.2in

Finally, we consider the case of a matrix equation
\begin{equation}\label{eqn:inv}
A*{\bf x} = {\bf b}
\end{equation}
when $A$ is invertible. If we assume ${\bf x}_0$ is a solution, we can multiply both sides of the equation on the left by $A^{-1}$ to get
\[
{\bf x}_0 = I*{\bf x}_0 = (A^{-1}*A)*{\bf x}_0 = A^{-1}*(A*{\bf x}_0) = A^{-1}*{\bf b}
\]
On the other hand, if we take $x = A^{-1}*{\bf b}$ and substitute into equation (\ref{eqn:inv}), we get
\[
A*(A^{-1}*{\bf b}) = (A*A^{-1})*{\bf b} = I*{\bf b} = {\bf b}
\]
In other words, we have shown

\begin{theorem} If $A$ is an invertible $n\times n$ matrix, then for any $n\times 1$ vector ${\bf b}$ and $n\times 1$ vector variable ${\bf x}$, the matrix equation
\[
A*{\bf x} = {\bf b}
\]
is consistent, and has a unique solution given by ${\bf x} = A^{-1}*{\bf b}$.
\end{theorem}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The superposition principle} Given a matrix equation $A*{\bf x} = {\bf b}$, its {\it associated homogeneous equation} is $A*{\bf x} = {\bf 0}$ (resulting from replacing $\bf b$ by $\bf 0$). Note that consistency status may change in passing from an arbitrary non-homogeneous equation to its associated counterpart. However, if the original system is consistent, there is an important relation between the two solution sets, which is a manifestation of the {\it superposition principle}.
\vskip.2in

To see this, note that if ${\bf x}', {\bf x}''$ are two solutions to the equation $A*{\bf x} = {\bf b}$, then 
\[
A*({\bf x}' - {\bf x}'') = A*{\bf x}' - A*{\bf x}'' = {\bf b} - {\bf b} = {\bf 0}
\]
so $({\bf x}' - {\bf x}'')$ is a solution to the associated homogeneous equation. On the other hand, given a solution ${\bf x}_h$ to the associated homogeneous equation, and a solution ${\bf x}$ to the original equation, we see
\[
A*({\bf x}_h + {\bf x}) = A*{\bf x}_h + A*{\bf x} = {\bf 0} + {\bf b} = {\bf b}
\]
so ${\bf x}_h + {\bf x}$ is again a solution to the original equation. Thus

\begin{thm} (Superposition Principle) Suppose $A*{\bf x} = {\bf b}$ is a consistent matrix equation, with ${\bf x}_p$ a particular solution to the equation. Then the set of solutions to the original equation can be expressed as
\[
\{{\bf x}_h + {\bf x}_p\ |\ {\bf x}_h\in S_0\}
\]
where $S_0$ denotes the set of solutions to the associated homogeneous equation.
\end{thm}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Elementary matrices} As we have seen, systems of equations - or equivalently matrix equations - are solved by i) forming the ACM associated with the set of equations and ii) applying row operations to the ACM until it is in reduced row echelon form.
\vskip.2in
It turns out these row operations can be realized by left multiplication by a certain type of matrix, and these matrices have uses beyond that of performing row operations. To explain how matrix multiplication comes into play, let us write $\cal R(_-)$ for a particular row operation on $m\times n$ matrices, so that the given operation is represented by $A\mapsto {\cal R}(A)$. It turns out that for any of the three types of row operations we have considered above, one has the identity
\[
{\cal R}(A) = {\cal R}(I*A) = {\cal R}(I)*A
\]
In other words, {\it the row operation ${\cal R}(_-)$, applied to $A$, can be realized in terms of left multiplication by the $m\times m$ matrix ${\cal R}(I)$ gotten by applying $\cal R$ to the $m\times m$ identity matrix}.
\vskip.2in

As one would then expect, one has - for each row operation - a corresponding {\it elementary matrix} derived from the identity matrix of the appropriate dimension by application of that given operation.
\vskip.2in

These matrices, and the notation used to define them, can be recorded in an expanded version of the table above in which we indicated the types of operations and their representation:
\vskip.2in

\begin{center}
    \begin{tabular}{|c|c|c|c|}\hline\hline
    \phantom{x} & \phantom{x} & \phantom{x} & \phantom{x}\\
    \Large{Type} &\Large{What it does} &\Large{Indicated by} & \Large{Elementary matrix}\\ \hline
    \phantom{x} & {\large Switches} & \phantom{\Huge X} & \phantom{X}\\
    \large{Type I} &{\large $i^{th}$ and $j^{th}$} & {\large $R_i\leftrightarrow R_j$} & {\large ${\cal R}(I) = P_{ij}$}\\ 
    \phantom{x} & {\large rows} & \phantom{x} & \phantom{x}\\ \hline
    \phantom{x} &{\large Multiplies} & \phantom{\Huge X} & \phantom{x}\\
    \large{Type II} &{\large $i^{th}$ row} & {\large $r\cdot R_i$} & {\large ${\cal R}(I) = D_i(r)$}\\
    \phantom{x} & {\large by $r\ne 0$} & \phantom{x} & \phantom{x}\\ \hline
    \phantom{x} &{\large Adds} & \phantom{\Huge X} & \phantom{x}\\
    \large{Type III} &{\large $a$ times the $i^{th}$ row} & {\large $a\cdot R_i$ added to $R_j$} & {\large ${\cal R}(I) = E_{ji}(a)$}\\
    \phantom{x} & {\large to the $j^{th}$ row} & \phantom{x} & \phantom{x}\\\hline\hline
    \end{tabular}
    \end{center}
\vskip.2in

[Examples to be included]
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Column operations} In an analogous fashion, one can also perform column operations on a matrix. As with row operations there are three types, and each type can be achieved via {\it right} multiplication with the corresponding matrix. In other words, if ${\cal C}(_-)$ indicates the given column operation, then for each type one has
\[
{\cal C}(A) = {\cal C}(A*I) = A*{\cal C}(I)
\]
Denoting the $k^{th}$ column by $C_k$, we have
\vskip.2in

\begin{center}
    \begin{tabular}{|c|c|c|c|}\hline\hline
    \phantom{x} & \phantom{x} & \phantom{x} & \phantom{x}\\
    \Large{Type} &\Large{What it does} &\Large{Indicated by} & \Large{Elementary matrix}\\ \hline
    \phantom{x} & {\large Switches} & \phantom{\Huge X} & \phantom{X}\\
    \large{Type I} &{\large $i^{th}$ and $j^{th}$} & {\large $C_i\leftrightarrow C_j$} & {\large ${\cal C}(I) = P_{ij}$}\\ 
    \phantom{x} & {\large colmns} & \phantom{x} & \phantom{x}\\ \hline
    \phantom{x} &{\large Multiplies} & \phantom{\Huge X} & \phantom{x}\\
    \large{Type II} &{\large $i^{th}$ column} & {\large $r\cdot C_i$} & {\large ${\cal C}(I) = D_i(r)$}\\
    \phantom{x} & {\large by $r\ne 0$} & \phantom{x} & \phantom{x}\\ \hline
    \phantom{x} &{\large Adds} & \phantom{\Huge X} & \phantom{x}\\
    \large{Type III} &{\large $a$ times the $i^{th}$ column} & {\large $a\cdot C_i$ added to $C_j$} & {\large ${\cal C}(I) = E_{ij}(a)$}\\
    \phantom{x} & {\large to the $j^{th}$ column} & \phantom{x} & \phantom{x}\\\hline\hline
    \end{tabular}
    \end{center}
\vskip.2in

[Examples to be included]
\vskip.2in

Each elementary matrix is invertible, and of the same type. The following indicates how each elementary matrix behaves under i) inversion and ii) transposition:
\begin{gather*}
P_{ij}^{-1} = P_{ij},\qquad P_{ij}^T = P_{ij}\\
D_i(r)^{-1} = D_i(r^{-1}),\qquad D_i(r)^T = D_i(r)\\
E_{ij}(a)^{-1} = E_{ij}(-a),\qquad E_{ij}(a)^T = E_{ji}(a)
\end{gather*}

This is useful in problems where one wants to express the inverse of a matrix explicitly as a product of elementary matrices.







\vskip.5in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Vector spaces}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{$\mathbb R^n$} Recall from above that $\mathbb R^{m\times n}$ denotes the set of all $m\times n$ matrices with real entries, and that the elements of this set are called row resp.\ column vectors when $m=1$ resp.\ $n=1$. Our convention will be to denote $\mathbb R^{m\times 1}$ as simply $\mathbb R^m$. In other words, a {\it real m-dimensional vector} will always refer to an $m\times 1$ real column vector (for reasons of spatial economy, though, when writing an element of $\mathbb R^m$ in coordinate form, we will often express it as the transpose of a row vector).
\vskip.2in

From the definition of matrix addition and scalar multiplication, we see that we can i) add two vectors together, and ii) multiply a vector by a scalar, with the result being a (possibly different) vector in the same space that we started with. In other words,
\vskip.1in

\begin{itemize}
\item[[C1]] (Closure under vector addition) Given ${\bf v}, {\bf w}\in\mathbb R^n$, ${\bf v} + {\bf w}\in\mathbb R^n$.
\item[[C2]] (Closure under scalar multiplication) Given ${\bf v}\in\mathbb R^n$ and $\alpha\in\mathbb R$, $\alpha{\bf v}\in\mathbb R^n$.
\end{itemize}
\vskip.1in

Moreover, the space $\mathbb R^n$ equipped with these two operations satisfies certain fundamental properties. In what follows, $\bf u$, $\bf v$, $\bf w$ denote arbitrary vectors in $\mathbb R^n$, while $\alpha,\beta$ represent arbitrary scalars in $\mathbb R$.

\begin{itemize}
\item[[A1]] (Commutativity of addition) ${\bf v} + {\bf w} = {\bf w} + {\bf v}$.
\item[[A2]] (Associativity of addition) $({\bf u} + {\bf v}) + {\bf w} = {\bf u} + ({\bf v} + {\bf w})$.
\item[[A3]] (Existence of a zero vector) There is a vector ${\bf z}$ with ${\bf z} + {\bf v} = {\bf v} + {\bf z} = {\bf v}$.
\item[[A4]] (Existence of additive inverses) For each $\bf v$, there is a vector $-{\bf v}$ with ${\bf v} + (-{\bf v}) = (-{\bf v}) + {\bf v} = {\bf z}$.
\item[[A5]] (Distributivity of scalar multiplication over vector addition) $\alpha({\bf v} + {\bf w}) = \alpha{\bf v} + \alpha{\bf w}$.
\item[[A6]] (Distributivity of scalar addition over scalar multiplication) $(\alpha + \beta){\bf v} = \alpha{\bf v} + \beta{\bf v}$.
\item[[A7]] (Associativity of scalar multiplication) $(\alpha \beta){\bf v}) = (\alpha(\beta {\bf v})$.
\item[[A8]] (Scalar multiplication with 1 is the identity) $1{\bf v} = {\bf v}$.
\end{itemize}
\vskip.2in

We should briefly mention why $\mathbb R^n$ satisfies these properties. First, the definition of matrix addition and scalar multiplication imply [C1] and [C2]. The properties [A1] - [A8], excepting [A3] and [A4],are  a consequence of Theorem \ref{thm:matalg}. The so-called {\it existential} properties (referring to the fact they claim the existence of certain vectors) follow by direct observation. 
\vskip.2in
These properties isolate the fundamental algebraic structure of $\mathbb R^n$, and lead to the following definition (one of the most central in all of linear algebra).
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definition of a vector space}

\begin{definition} A {\it vector space} is a set $V$ equipped with two operations - vector addition \lq\lq$+$\rq\rq\ and scalar multiplication \lq\lq$\cdot$\rq\rq - which satisfy the closure axioms for $V$:
\begin{itemize}
\item[[C1]] (Closure under vector addition) Given ${\bf v}, {\bf w}\in V$, ${\bf v} + {\bf w}\in V$.
\item[[C2]] (Closure under scalar multiplication) Given ${\bf v}\in V$ and a scalar $\alpha$, $\alpha{\bf v}\in V$.
\end{itemize}
together with the eight {\it vector space axioms} [A1] - [A8].
\end{definition}

In this way, a vector space should properly be represented as a triple $(V,+,\cdot)$, to emphasize the fact that the algebraic structure depends not just on the underlying set of vectors, but on the choice of operations representing addition and scalar multiplication.
\vskip.2in

\begin{example} Let $V = \mathbb R^{m\times n}$, the space of $m\times n$ matrices, with addition given by matrix addition and scalar multiplication as defined for matrices. Then $(\mathbb R^{m\times n},+,\cdot)$ is a vector space. Again, as with $\mathbb R^n$, the closure axioms are seen to be satisfied as a direct consequence of the definitions, while the other properties follow from Theorem \ref{thm:matalg} together with direct construction of the $m\times n$ \lq\lq zero vector\rq\rq\ $0^{m\times n}$, as well as additive inverses as indicated in [A4].
\end{example}

Before proceeding to other examples, we need to discuss an important point regarding how theorems about vector spaces are typically proven. In any system of mathematics, one operates with a certain set of assumptions, called {\it axioms}, together with various results previously proven (possibly in other areas of mathematics) and which one is allowed to assume true without further verification.
\vskip.2in

In the case of {\it vector spaces over $\mathbb R$} (i.e. where the scalars are real numbers), the standing assumption is that the above list of ten properties hold for the real numbers. The fastidious reader will note that this was already assumed in the proof of Theorem \ref{thm:matalg}; in fact the proof of that theorem would have been impossible without such an assumption. To illustrate how this foundational assumption applies in a different context, we consider the space
\[
F[a,b] = \ \text{the space of real-valued functions on the closed interval }[a,b]
\]
Recall that i) a function is completely determined by the values it takes on the elements of its domain, and therefore ii) two functions $f, g$ are {\it equal} iff they have the same domain and $f(x) = g(x)$ for all elements $x$ in their common domain. So in order to show two functions $f$ and $g$ on the closed interval $[a,b]$ are equal, it suffices to verify that $f(c) = g(c)$ for all $a\le c\le b$.
\vskip.2in

Next recall that the sum of two functions is given by
\[
(f+g)(x) := f(x) + g(x)
\]
while the scalar multiple $\alpha f$ of the function $f$ is given by
\[
(\alpha f)(x) := \alpha(f(x))
\]

\begin{theorem} Equipped with addition and scalar multiplication as just defined, $(F[a,b],+,\cdot)$ is a vector space.
\end{theorem}


\begin{proof} One begins by verifying the two closure axioms. If $f,g\in F[a,b]$, they are real-valued functions with common domain $[a,b]$; hence their sum is defined by the above equation, and has the same domain, making $f+g$ a function in $F[a,b]$. Similarly, if $f\in F[a,b]$ and $\alpha\in\mathbb R$, then multiplying $f$ by $\alpha$ leaves the domain unchanged, so $\alpha f\in F[a,b]$.
\vskip.2in
The eight vector space axioms [A1] - [A8] are of two types. The third and fourth are {\it existential} - they assert the existence of the zero element and additive inverses, respectively. To verify these, one simply has to produce the candidate satisfying the requisite properties. The remaining six are {\it universal}. They involve statements which hold for all collections of vectors for which the given equality makes sense. We will verify each of the eight axioms in detail. This example, then, can be used as a template for how to proceed in other cases with verification that a proposed candidate vector space is in fact one. 
\vskip.2in

[A1]: For all $f,g\in F[a,b]$ and $x\in [a,b]$,
\begin{align*}
(f+g)(x) &= f(x) + g(x)\quad\text{by definition of addition for functions}\\
             &= g(x) + f(x)\quad\text{by commutativity of addition for real numbers}\\
             &=(g+f)(x)\quad\text{by definition of addition for real numbers}
\end{align*}
\vskip.2in

[A2]: For all $f,g,h\in F[a,b]$ and $x\in [a,b]$,
\begin{align*}
((f+g)+h)(x) &= (f+g)(x) + h(x)\quad\text{by definition of addition for functions}\\
                     &= (f(x) + g(x)) + h(x)\quad\text{by definition of addition for functions}\\
                     &=f(x) + (g(x)) + h(x))\quad\text{by associativity of addition for real numbers}\\
                     &=f(x) + (g+h)(x)\quad\text{by definition of addition for functions}\\
                     &= (f+(g+h))(x)\quad\text{by definition of addition for functions}
\end{align*}
\vskip.2in

[A3]: Define $z\in F[a,b]$ by $z(x) = 0, a\le x\le b$. Then for all $f\in F[a,b]$ and $x\in [a,b]$, 
\begin{align*}
(z+f)(x) &= z(x) + f(x)\quad\text{by definition of addition for functions}\\
             &= 0 + f(x)\quad\text{by definition of $z$}\\
             &= f(x)\quad\text{by the defining property of $0\in\mathbb R$}\\
             &= f(x) + 0\quad\text{by the defining property of $0\in\mathbb R$}\\
             &= f(x) + z(x)\quad\text{by definition of $z$}\\
             &= (f + z)(x)\quad\text{by definition of addition for functions}
\end{align*}
\vskip.2in

[A4]: For each $f\in F[a,b]$, define $(-f)(x) := -(f(x))$ (note the different placement of parentheses on the two sides of the equation). Then for all $f\in F[a,b]$ and $x\in [a,b]$, 
\begin{align*}
(f + (-f))(x) &= f(x) + (-f)(x)\quad\text{by definition of addition for functions}\\
             &= f(x) + (-f(x))\quad\text{by definition of $(-f)$}\\
             &= 0\quad\text{by the property of additive inverses in $\mathbb R$}\\
             &= z(x)\quad\text{by the definition of $z\in F[a,b]$}\\
             &= 0\quad\text{by the definition of $z\in F[a,b]$}\\
             &= -f(x) + f(x)\quad\text{by the property of additive inverses in $\mathbb R$}\\
             &= ((-f) + f)(x)\quad\text{by definition of addition for functions}
\end{align*}
\vskip.2in

[A5]: For all $f,g\in F[a,b]$ and $\alpha, x\in [a,b]$,
\begin{align*}
(\alpha (f+g))(x) &= \alpha((f+g)(x))\quad\text{by definition of scalar multiplication for functions}\\
             &= \alpha(f(x) + g(x))\quad\text{by definition of addition for functions}\\
             &= \alpha f(x) + \alpha g(x)\quad\text{by distributivity of multiplication over addition in $\mathbb R$}\\
             &= (\alpha f)(x) + (\alpha g)(x)\quad\text{by definition of scalar multiplication for functions}\\
             &= ((\alpha f) + (\alpha g))(x)\quad\text{by definition of addition for functions}
\end{align*}
\vskip.2in

[A6]: For all $f\in F[a,b]$ and $\alpha,\beta, x\in [a,b]$,
\begin{align*}
((\alpha +\beta)f)(x) &= (\alpha +\beta)f(x)\quad\text{by definition of scalar multiplication for functions}\\
             &= \alpha f(x) + \beta f(x)\quad\text{by distributivity of multiplication over addition in $\mathbb R$}\\
             &= (\alpha f)(x) + (\beta f)(x)\quad\text{by definition of scalar multiplication for functions}\\
             &= ((\alpha f) + (\beta f))(x)\quad\text{by definition of addition for functions}
\end{align*}
\vskip.2in

[A7]: For all $f\in F[a,b]$ and $\alpha,\beta, x\in [a,b]$,
\begin{align*}
((\alpha \beta)f)(x) &= (\alpha \beta)f(x)\quad\text{by definition of scalar multiplication for functions}\\
             &= \alpha(\beta f(x))\quad\text{by associativity of multiplication in $\mathbb R$}\\
             &= \alpha ((\beta f)(x))\quad\text{by definition of scalar multiplication for functions}\\
             &= (\alpha(\beta f))(x)\quad\text{by definition of scalar multiplication for functions}
\end{align*}
\vskip.2in

[A8]: For all $f\in F[a,b]$ and $x\in [a,b]$,
\begin{align*}
(1\cdot f)(x) &= 1\cdot f(x)\quad\text{by definition of scalar multiplication for functions}\\
                     &= f(x)\quad\text{by the multiplicative property of $1\in\mathbb R$}
\end{align*}
\end{proof}
\vskip.2in

\begin{exercise} Show that $(\mathbb R^{m\times n}, +,\cdot)$ is a vector space, where \lq\lq +\rq\rq\ denotes matrix addition, and \lq\lq$\cdot$\rq\rq\ denotes scalar multiplication for matrices (hint: use the results of Theorem \ref{thm:matalg}).
\end{exercise}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Linear combinations and linear independence} The operation of forming linear combinations of vectors is at the heart of Linear Algebra; it is, arguably, the central construct of the entire subject. And yet, it is relatively straightforward to describe.
\vskip.2in

\begin{definition} Let $(V,+,\cdot)$ be a vector space, and ${\bf v}_1,\dots, {\bf v}_n\in V$ a collection of $n$ vectors in $V$. Then a {\it linear combination} of ${\bf v}_1,\dots,{\bf v}_n$ is a sum of scalar multiples of these vectors; in other words, a sum of the form
\begin{equation}
\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2 +\dots + \alpha_n{\bf v}_n
\end{equation}
for some choice of scalars $\alpha_1,\alpha_2,\dots,\alpha_n$. A vector ${\bf v}$ is a linear combination of ${\bf v}_1,\dots,{\bf v}_n$ if it can be written in this form. 
\end{definition}

\begin{example} Suppose ${\bf v}_1 = [1\ 0\ 0]^T, {\bf v}_2 = [0\ 1\ 0]^T\in\mathbb R^3$ (we have written these column vectors as transposed row vectors). Then ${\bf v} = [2\ 5\ 0]^T = 2{\bf v}_1 + 5{\bf v}_2$, so ${\bf v}$ is a linear combination of ${\bf v}_1,{\bf v}_2$. On the other hand, ${\bf w} = [0\ 0\ 1]^T$ is {\it not} a linear combination of ${\bf v}_1,{\bf v}_2$, since the $(3,1)$-entry of ${\bf w}$ is non-zero, while any linear combination of ${\bf v}_1,{\bf v}_2$ would have a $(3,1)$-entry equal to $0$, regardless of the choice of scalars for coefficients.
\end{example}

In general, given vectors ${\bf v}_1,{\bf v}_2,\dots,{\bf v}_n$, it can be quite difficult to determine simply by inspection whether or not some other vector ${\bf v}$ is or is not a linear combination of the given collection. One of our goals, discussed in detail below, will be to establish some systematic way of answering this question.

\begin{definition} A collection of vectors ${\bf v}_1,\dots,{\bf v}_n$ is {\it linearly independent} if
\begin{equation}
\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2 +\dots + \alpha_n{\bf v}_n = {\bf z}\qquad \boldsymbol{\Longleftrightarrow}\qquad \alpha_1 = \alpha_2 = \dots = \alpha_n = 0
\end{equation}
In other words, the only linear combination of the vectors that produces the zero vector is the {\it trivial} combination, where each coefficient $\alpha_i = 0$.
\end{definition}

Call $\{{\bf v}_1,\dots,{\bf v}_n\}$ {\it vectorwise independent} if no vector in the set can be written as a linear combination of the remaining vectors in the set. The following lemma records the equivalence of these two concepts.


\begin{lemma} A collection of vectors $\{{\bf v}_1,\dots,{\bf v}_n\}$ is linearly independent iff it is vectorwise independent.
\end{lemma}

\begin{proof} Suppose $\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2 +\dots + \alpha_n{\bf v}_n = {\bf z}$ is a linear combination of $\{{\bf v}_1,\dots,{\bf v}_n\}$ equalling the zero vector. If $\alpha_i\ne 0$ for some $1\le i\le n$, then this linear relation may be rewritten as
\[
{\bf v}_i = \frac{-\alpha_1}{\alpha_i}{\bf v}_1 + \frac{-\alpha_2}{\alpha_i}{\bf v}_2 +\dots + \frac{-\alpha_{i-1}}{\alpha_i}{\bf v}_{i-1} + \frac{-\alpha_{i+1}}{\alpha_i}{\bf v}_{i+1}+\dots + \frac{-\alpha_n}{\alpha_i}{\bf v}_n 
\]
from which we see that linear dependence implies the set is not vectorwise independent, or (via the contrapositive), (vectorwise independence)$\Rightarrow$(linear independence). On the other hand, suppose the vectors are vectorwise dependent. Thus there must exist an index $i$ and scalars $\beta_j, j\ne i$ such that
\[
{\bf v}_i = \beta_1{\bf v}_1 + \beta_2{\bf v}_2 +\dots + \beta_{i-1}{\bf v}_{i-1} + \beta_{i+1}{\bf v}_{i+1}+\dots + \beta_n{\bf v}_n 
\]
or, equivalently
\[
{\bf z} = \beta_1{\bf v}_1 + \beta_2{\bf v}_2 +\dots + \beta_{i-1}{\bf v}_{i-1} - {\bf v}_i + \beta_{i+1}{\bf v}_{i+1}+\dots + \beta_n{\bf v}_n 
\]
from which we see that (vectorwise dependence)$\Rightarrow$(linear dependence) or, again by contraposition, (linear independence)$\Rightarrow$(vectorwise independence).
\end{proof}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Subspaces} If $V = \{V,+,\cdot\}$ is a vector space, then $W\subseteq V$ is called a {\it subspace} of $V$ if the restriction to $W$ of the sum and scalar product operations of $V$ make $W$ a vector space. It might seem that, in order to show some collection of vectors in $V$ form a subspace, one would have to go through the entire list of axioms checking each one. In fact, one only needs to check the closure axioms.

\begin{theorem} Let $W$ be a subset of vectors in $V$, which satisfy the two closure axioms C1 and C2 with respect to the operations on $V$. Then $W$ is a subspace of $V$.
\end{theorem}

\begin{proof} As we have already observed, the vector space axioms A1 - A8 fall into two types: {\it existential} (claiming the existence of certain vectors), and {\it universal} (indicating some property is universally true). Of these eight axioms, all but A3 and A4 are of the second type. These are automatically satisfied for vectors in $W$, because they hold for the larger space $V$ in which $W$ lies. In other words, for these six axioms, there is nothing to prove. 
\vskip.2in

The issue is with A3 and A4. To this end, we first show

\begin{claim} For any vector ${\bf v}\in V$, ${\bf z} = 0{\bf v}$. In addition, $-{\bf v} = (-1){\bf v}$.
\end{claim}
In other words, the zero vector of $V$ can be realized by taking any vector in $V$ and multiplying it by the scalar $0$, while the additive inverse of any vector can be gotten by multiplying it by the scalar $-1$.

\begin{proof} Fix ${\bf v}\in V$. Then
\[
{\bf v} + 0{\bf v} = 1{\bf v} + 0{\bf v} = (1+0){\bf v} = 1{\bf v} = {\bf v}
\]
so adding $-{\bf v}$ to both sides gives
\[
0{\bf v} = z + 0{\bf v} = (-{\bf v} + {\bf v}) + 0{\bf v} = -{\bf v} + ({\bf v} + 0{\bf v}) = -{\bf v} + {\bf v} = z
\]
verifying the first claim. Knowing this, we then have
\[
(-1){\bf v} + {\bf v} = (-1){\bf v} + 1{\bf v} = (-1+1){\bf v} = 0{\bf v} = z
\]
implying $(-1){\bf v} = -{\bf v}$ by the uniqueness of the additive inverse indicated by A4. 
\end{proof}

Axioms A3 and A4 then follow immediately for $W$, by virtue of the fact that $W$ is closed under scalar multiplication.
\end{proof}

There are some important and basic constructions that allow one to define subspaces.

\subsubsection{Spanning sets} If $S = \{{\bf v}_1,\dots, {\bf v}_n\}\subset V$ is a (finite) collection of vectors in a vector space $V$, then the {\it span} of $S$ is the set of all linear combinations of the vectors in $S$. That is
\[
Span(S) := \{\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2 +\dots +\alpha_n{\bf v}_n\ |\ \alpha_i\in\mathbb R\}
\] 

\begin{remark} If $S = \{{\bf v}_i\ |\ i\in\mathbb N\}$ is a countably infinite set of vectors, then the (linear, algebraic) span of the vectors is defined to be
\[
Span\{{\bf v}_i\ |\ i\in\mathbb N\} := \left\{\sum_i\alpha_i{\bf v}_i\ |\ \text{all but finitely many of the }\alpha_i\text{ are zero}\right\}
\]
the definition can be extended to arbitrarily large sets of vectors using a slightly different method of extension. Thus, if $S\subseteq V$ is an arbitrary set of vectors in $V$, then
\[
Span(S) := \underset{T\text{ finite}}{\underset{T\subseteq S}{\bigcup}} Span(T)
\]
\end{remark}

\begin{definition} If $V$ is a vector space, and $S$ a set of vectors in $V$, then we say that $S$ is a {\it spanning set} for $V$ if $V = Span(S)$.
\end{definition}

\begin{exercise} Show that for any (non-empty) set of vectors $S\subset V$, $Span(S)$ is a subset of $V$ (in other words, it is closed under the addition and scalar multiplication operations coming from $V$). Your argument should work for general sets $S$ without any assumptions on cardinality.
\end{exercise}

\begin{lemma} Every vector space $V$ has a spanning set.
\end{lemma}

\begin{proof} Because we allow spanning sets to be arbitrarily large, we can take $S = V$ and observe that $V = Span(V)$ for trivial reasons.
\end{proof}

This lemma suggests that spanning sets are not only not unique, they can have vastly different sizes. For example, $\mathbb R^2$ is spanned by $\{{\bf e}_1, {\bf e}_2\}$. It is also spanned by the set $\mathbb R^2$ itself, which is much larger. It is natural to ask how small a spanning set can be. This leads to

\begin{definition} $S$ is a {\it minimal spanning set} for $V$ if
\begin{itemize} 
\item $V = Span(S)$, and
\item For any proper subset $T\subsetneq S$, $Span(T)\subsetneq V$.
\end{itemize}
\end{definition}


Minimal spanning sets play a very important role in the study of vector spaces, and are discussed in greater detail below.
\vskip.3in

\subsubsection{Nullspaces} If $A$ is an $m\times n$ matrix with entries in $\mathbb R$, the {\it nullspace of A} is the set
\[
N(A) := \{{\bf x}\in\mathbb R^n\ |\ A*{\bf x} = {\bf 0}\}\subset \mathbb R^n
\]
In other words, $N(A)$ is the set of all solutions to the homogeneous matrix equation $A*{\bf x} = {\bf 0}$.

\begin{lemma} For any $m\times n$ matrix $A$ with entries in $\mathbb R$, $N(A)$ is a subspace of $\mathbb R^n$.
\end{lemma}

\begin{proof} Note that ${\bf 0}\in N(A)$. So we need to show that $N(A)$ satisfies the two closure axioms.
\vskip.1in

{\bf\underbar{C1}} Suppose ${\bf v}, {\bf w}\in N(A)$. Then
\[
A*({\bf v} + {\bf w}) = A*{\bf v} + A*{\bf w} = {\bf 0} + {\bf 0} = {\bf 0}
\]
Hence $N(A)$ is closed under addition.
\vskip.1in

{\bf\underbar{C2}} Suppose $\alpha\in\mathbb R$ and ${\bf v}\in N(A)$. Then
\[
A*(\alpha{\bf v}) = \alpha(A*{\bf v}) = \alpha{\bf 0} = {\bf 0} 
\]
Hence $N(A)$ is closed under scalar multiplication. This completes the proof.
\end{proof}
\vskip.3in


\subsubsection{Codomains} Let $A$ be as above. Then the {\it codomain} or {\it range} of $A$ (viewed as a linear transformation, defined below) is
\[
Range(A) := \{{\bf b}\in\mathbb R^m\ |\ A*{\bf x} = {\bf b}\ \text{ is consistent}\}\subseteq \mathbb R^m
\]

In analogy to the nullspace, we have

\begin{lemma} For any real $m\times n$ matrix $A$, $Range(A)$ is a subspace of $\mathbb R^m$.
\end{lemma}

\begin{proof} Again, we first note that ${\bf 0}\in Range(A)$, as $A*{\bf 0} = {\bf 0}$. As for the closure axioms,
\vskip.1in

{\bf\underbar{C1}} Suppose ${\bf v}, {\bf w}\in Range(A)$. Choose ${\bf x},{\bf y}\in\mathbb R^n$ satsifying $A*{\bf x} = {\bf v}, A*{\bf y} = {\bf w}$. Then
\[
A*({\bf x} + {\bf y}) = A*{\bf x} + A*{\bf y} = {\bf v} + {\bf w}
\]
implying $Range(A)$ is closed under addition.
\vskip.1in

{\bf\underbar{C2}} Suppose $\alpha\in\mathbb R$ and $A*{\bf x} = {\bf v}\in Range(A)$. Then
\[
A*(\alpha{\bf x}) = \alpha(A*{\bf x}) = \alpha{\bf v} 
\]
implying $N(A)$ is closed under scalar multiplication.
\end{proof}
\vskip.3in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Bases and dimension} A set of vectors $S = \{{\bf v}_1,\dots, {\bf v}_n\}\subset V$ is a {\it basis} for $V$ if
\begin{itemize}
\item it spans $V$ and
\item it is linearly independent.
\end{itemize}

\vskip.2in

{\bf\underbar{Fundamental Properties}}

\begin{itemize}
\item[[B1]] $S = \{{\bf v}_1,\dots, {\bf v}_n\}\subset V$ is a basis for $V$ iff it is a minimal spanning set for $V$.
\item[[B2]] Every non-zero vector space $V$ admits a basis.
\item[[B3]] If $S = \{{\bf v}_1,\dots, {\bf v}_n\}$ and $T = \{{\bf w}_1,\dots, {\bf w}_m\}$ are two bases for $V$, then $m=n$.
\end{itemize}
\vskip.2in

{\bf\underbar{Proof of B1}}\, Suppose $S$ is a basis for $V$. Then it certainly spans $V$. If it were not a minimal spanning set, it would mean there is a vector ${\bf v}_0\in S$ which is in the span of $S\backslash\{{\bf v}_0\}$, which in turn would mean that ${\bf v}_0$ could be written as a linear combination of vectors in $S\backslash\{{\bf v}_0\}$. Thus the original set $S$ would not be vectorwise independent, which by the above exercise is a contradiction. Hence it must be minimal.
\vskip.1in
In the other direction, suppose it is a minimal spanning set. If it were not linearly independent, then by the same exercise it would not be vectorwise independent, so there would exist some vector ${\bf v}_1\in S$ which could be written as a linear combination of the other vectors in $S$. Then $Span(S) = Span(S\backslash\{{\bf v}_1\}$, contradicting the fact that $S$ is minimal. Hence $S$ not only spans but must also be linearly independent.\hfill$\square$
\vskip.2in

{\bf\underbar{Proof of B2}}\, Given the first property, there are two different ways one can go about constructing a basis. We present both. Note: they do require familiarity with the operations of union and intersection for sets.
\vskip.05in

\underbar{Method 1} Let $T_s = \{S\subset V\ |\ V = Span(S)\}$. We can partially order $T_s$ by defining $S\le S'$ if $S'\subset S$. Every chain is clearly bounded above as a set by $\empty$ (even though the bound is not an element of $T_s$). Choose a totally ordered subset of $T_s$; by Zorn's Lemma it contains a maximal element $S_m\in T_s$. Maximality with respect to this total ordering means $S_m$ is a minimal spanning set, hence a basis by {\bf B1}.
\vskip.05in

\underbar{Method 2} Let $T_l = \{S\subset V\ |\ S\ \text{is linearly independent}\}$, and partially order $T_l$ by $S\le S'$ iff $S\subset S'$. Again, every chain has an upper bound $S=V$ (which again will not be in $T_l$). Choose a totally ordered subset of $T_l$ and let $S_l$ be a maximal element, implying it is a maximal linearly independent subset of $V$, hence a basis.
\vskip.2in

The third property will require a bit more work; its proof will be deferred until later.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Vector spaces over $\mathbb C$} The complex numbers $\mathbb C$ are formed from the real numbers by adjoining $i = \sqrt{-1}$, so that $i^2 = -1$. Every complex number can b written uniquely as $z = a + bi$ where $a,b\in\mathbb R$. Every complex number has both a {\it real} and {\it imaginary} part, defined as
\[
Re(a+bi) = a,\qquad Im(a+bi) = b
\]
For $z = a + bi$, we recall its {\it conjugate} is defined to be $\ov{z} := a - bi$. The real numbers embed naturally in $\mathbb C$ as those whose {\it imaginary} component is zero: $\mathbb R\ni a\mapsto a + 0i\in\mathbb C$. Alternatively $\mathbb R$ identifies with those $z\in\mathbb C$ satisfying $z = \ov{z}$.
\vskip.2in

A {\it vector space over $\mathbb C$} satisfies exactly the same axioms as a vector space over $\mathbb R$, the one difference being that scalars are allowed to be compBeyond that, all of the constructions and definitions over $\mathbb R$ given above extend without change to working over $\mathbb C$.. In analogy to $\mathbb R^n$ we have $\mathbb C^n$, which can be viewed as the complex vector space generated by the same standard basis vectors we had for $\mathbb R^n$; vectors in $\mathbb C^n$ are naturally represented by $n\times 1$ column vectors with entries in $\mathbb C$:
\[
\mathbb C^n := \{[z_1\ z_2\ \dots z_n]^T\ |\ z_i\in\mathbb C\}
\]
In cases which involve both real and complex scalars, one has to take care as to which set of numbers one is working over, because this will make a difference when computing quantities such as dimension. To illustrate, $\mathbb C^1$ is a vector space over $\mathbb C$ with dimension $1$. On the other hand, because $\mathbb R\subset\mathbb C$, we could also consider $\mathbb C^1$ as a vector space over $\mathbb R$ by {\it restriction of scalars}; in other words, by only allowing scalar multiplication by real numbers. Over $\mathbb R$, $\mathbb C^1$ has dimension 2, not 1, with basis $\{[1], [i]\}$. Because of this it is not unusual (when discussing dimension) to emphasize the {\it base field} when there is any possibility of confusion or abiguity. In general, for any finite dimensional vector space $V$ over $\mathbb C$, if $Dim_{\mathbb C}(V) = n$ then $Dim_{\mathbb R}(V) = 2n$.
\vskip.5in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Coordinate systems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Coordinate vectors} From the beginning we have adopted the convention that vectors in $\mathbb R^n$ are represented by $n\times 1$ matrices - column vectors - with entries in $\mathbb R$. Another, equivalent way to arrive at such a description is to start with the {standard basis} $\underline{e} := \{{\bf e}^n_1, {\bf e}^n_2,\dots, {\bf e}^n_n\}$, viewed simply as a lnearly independent set, and then define $\mathbb R^n$ to be the span of these basis vectors. In this way, we realize that when writing down an $n\times 1$ column vector representing ${\bf v}$, we are simply recording (in columnar form) the coefficients that occur when representing $\bf v$ as a linear combination of standard basis vectors:
\[
{\bf v} = [a_1\ a_2\dots a_n]^T\quad \Leftrightarrow\quad {\bf v} = a_1 {\bf e}^n_1 + a_2{\bf e}^n_2 +\dots + a_n{\bf e}^n_n
\]
We notice now that this can be done not just for the standard basis in $\mathbb R^n$, but for {\it any} basis in any vector space. For the purposes of this section and what follows, we will only be concerned with such representations in {\it finite dimensional} vector spaces (or subspaces). 

\begin{definition} If $\underline{b} = \{ {\bf u}_1, {\bf u}_2, \dots, {\bf u}_n\}$ is a basis for a (finite dimensional) vector space $V$, and ${\bf v}\in\mathbb V$, the {\it coordinate representation} ${}_{\underline{b}}{\bf v}$ of ${\bf v}$ with respect to the basis $\underline{b}$ is the $n\times 1$ column vector which records the unique set of coefficients needed to represent $\bf v$ as a linear combination of the basis vectors in $\underline{b}$:
\[
{}_{\underline{b}}{\bf v} = [a_1\ a_2\dots a_n]^T\quad \Leftrightarrow\quad {\bf v} = a_1 {\bf u}_1 + a_2{\bf u}_2 +\dots + a_n{\bf u}_n
\]
\end{definition}

It is important here to distinguish between i) a vector ${\bf v}\in V$ and ii) its coordinate representation with respect to a particular basis for $V$. For that reason, vectors in $\mathbb R^n$ - written as $n\times 1$ column vectors - may some times be written with the decoration indicating that we are really looking at the representation of the vector with respect to the standard basis.
\vskip.2in

\begin{example} Let ${\bf v}\in\mathbb R^n$ with ${}_{\underline{e}}{\bf v} = [2\ 3\ -1]^T$, and let $\{{\bf u}_1, {\bf u}_2, {\bf u}_3\}$ be the basis of $\mathbb R^3$ given by
\[
{}_{\underline{e}}{\bf u}_1 = [1\ 1\ 0]^T,\quad {}_{\underline{e}}{\bf u}_2 = [1\ 0\ 1]^T,\quad {}_{\underline{e}}{\bf u}_3 = [0\ 1\ 1]^T
\]
We wish to determine ${}_{\underline{b}}{\bf v}$, the coordinate representation of $\bf v$ with respect to the basis $\underline{b}$. In other words, solve for the coefficients in the vector equation
\[
{\bf v} = x_1 {\bf u}_1 + x_2{\bf u}_2  + x_3{\bf u}_3
\]
Referring back to the consistency theorem for systems of equations, we see that solving for ${\bf x} = [x_1\ x_2\ x_3]^T$ is equivalent to solving for $\bf x$ in the matrix equation $A*{\bf x} = {}_{\underline{e}}{\bf v}^T$, where $A$ is the $3\times 3$ matrix whose columns are $\{{\bf u}_1, {\bf u}_2,{\bf u}_3\}$, or more precisely $\{{}_{\underline{e}}{\bf u}_1, {}_{\underline{e}}{\bf u}_2,{}_{\underline{e}}{\bf u}_3\}$:
\[
\begin{bmatrix}
1 & 1 & 0\\
1 & 0 & 1\\
0 & 1 & 1
\end{bmatrix}
*
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix} =
\begin{bmatrix}
2\\
3\\
-1
\end{bmatrix}
\]
Forming the ACM and putting it into reduced row echelon form yields
\[
rref\left(
\begin{amatrix}{3}
1 & 1 & 0 & 2\\
1 & 0 & 1 & 3\\
0 & 1 & 1 & -1
\end{amatrix}
\right)
=
\begin{amatrix}{3}
1 & 0 & 0 & 3\\
0 & 1 & 0 & -1\\
0 & 0 & 1 & 0
\end{amatrix}
\]
from which we see that
\[
{\bf x} = [3\ -1\ 0]^T = {}_{\underline{b}}{\bf v}
\]
\end{example}

This last example illustrates one of the basic computations we will want to be able to do; given the coordinate description of a vector with respect to one basis, find its coordinate representation with respect to a (specified) different basis. Of course one needs to know a bit more about the second basis. But given that information, we would want a systematic way to proceed. It turns out that this is most easily answered within the more general framework of linear transformations and their matrix representations, which we discuss next.
\vskip.5in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Linear Transformations}

\subsection{Definition} Suppose $V$ and $W$ are vector spaces (of arbitrary dimension). A {\it function} $f:V\to W$ is (as we recall) a rule which associates to each ${\bf v}\in V$ a unique vector $f({\bf v})\in W$. We will call such a function a {\it linear transformation} if it commutes with the linear sturucture in the domain and range:

\begin{gather}
f({\bf v} + {\bf w}) = f({\bf v}) + f({\bf w})\qquad\forall {\bf v,w}\in V\\
f(\alpha{\bf v}) = \alpha f({\bf v})\qquad\qquad\forall \alpha\in\mathbb R, {\bf v}\in V
\end{gather}

In other words, $f$ takes sums to sums and scalar products to scalar products. These two properties can be combined into one, using linear combinations. Precisely, $f$ is a linear transformation if

\begin{equation}
f(\alpha{\bf v} + \beta{\bf w}) = \alpha f({\bf v}) + \beta f({\bf w})\qquad\forall \alpha,\beta\in\mathbb R, {\bf v}, {\bf w}\in V
\end{equation}

\begin{exercise} Show that $f$ is a linear transformation iff
\[
f(\alpha_1{\bf v}_1 +\dots \alpha_n{\bf v}_n) = \alpha_1 f({\bf v}_1) +\dots \alpha_n f({\bf v}_n)\qquad\forall \alpha_i\in\mathbb R, {\bf v}_i\in V
\]
\end{exercise}

As a consequence of this exercise, we have

\begin{corollary} If $f: V\to W$ is a linear transformation, then it is uniquely determined by its values on a basis for $V$. Conversely, if $S := \{{\bf v}_1,\dots {\bf v}_n\}$ is a basis for $V$ and $g:S\to W$ is a function from the set $S$ to $W$, then $g$ extends uniquely to a linear transformation $f:V\to W$ with $f({\bf v}_i) = g({\bf v}_i), 1\le i\le n$. Thus, if $f_1, f_2:V\to W$ are two linear transformations which agree on a basis for $V$, then $f_1 = f_2$.
\end{corollary}

We will often denote a linear transformation between two vector spaces by the letter $L$, when wanting to emphasize its linear nature.
\vskip.3in

\subsection{Matrix representations of transformations} Suppose $V = \mathbb R^n, W = \mathbb R^m$, and $L_A:V\to W$ is given by 
\[
L_A({\bf v}) = A*{\bf v}
\]
for some $m\times n$ real matrix $A$. Then it follows immediately from the properties of matrix algebra that $L_A$ is a linear transformation. Conversely, suppose the linear transformation $L$ is given. If we define a matrix by
\[
A_L = [L({\bf e}_1)\ L({\bf e}_2)\ \dots L({\bf e}_n)]
\]
that is, the $m\times n$ matrix with $A(:,i) = L({\bf e}_i),\, 1\le i\le n$. Then by construction
\[
A_L*({\bf e}_i) = A(:,i) = L({\bf e}_i),\, 1\le i\le n
\]
so that ${\bf v}\mapsto L({\bf v})$ and ${\bf v}\mapsto A_L*{\bf v}$ are two linear transformations which agree on a basis for $\mathbb R^n$, which by the previous corollary implies
\[
L({\bf v}) = A_L*({\bf v})\qquad \forall{\bf v}\in \mathbb R^n
\]
Because of this, the matrix $A_L$ is referred to as a {\it matrix representation} of $L$. Note that this representation is with respecto to the standard basis for $\mathbb R^n$ and $\mathbb R^m$.
\vskip.2in

We see now that the same type of representation applies for arbitrary vector spaces {\it once a basis has been fixed for both the domain and target}. Thus, given
\begin{itemize}
\item A vector space $V$ with basis $S = \{{\bf v}_1,\dots,{\bf v}_n\}$,
\item a vector space $W$ with basis $T = \{{\bf w}_1,\dots,{\bf w}_m\}$, and
\item a linear transformation $L:V\to W$
\end{itemize} 
we could ask if there is a similar reprensentation of $L$ in terms of a matrix (which depends on these two choices of bases). The answer is given by

\begin{theorem}\label{thm:matrep} For any ${\bf v}\in V$
\[
{}_TL({\bf v}) = {}_TL_S*{}_S{\bf v}
\]
where ${}_SL_T$ is the $m\times n$ matrix defined  by
\[
{}_TL_S = [{}_TL({\bf v}_1)\ {}_TL({\bf v}_2)\ \dots\ {}_TL({\bf v}_n)]
\]
\end{theorem}

\begin{proof} Again by the above corollary it suffices to verify the equality for basis vectors. But ${}_S{\bf v}_i$ is the $n\times 1$ coordinate vector identical to the basis vector ${\bf e}_i$ for $\mathbb R^n$. From this we get
\[
{}_TL({\bf v}_i) = {}_TL_S(:,i) = {}_TL_S*{}_S{\bf v}_i,\qquad 1\le i\le n
\]
completing the proof.
\end{proof}
\vskip.3in

\subsection{Change of basis} Suppose now that $V$ is an $n$-dimensional vector space equipped with two bases $S_1 = \{{\bf v}_1,{\bf v}_2,\dots,{\bf v}_n\}$ and $S_2 = \{{\bf w}_1, {\bf w}_2,\dots,{\bf w}_n\}$ (we are assuming here the fact, listed above, that any two bases for $V$ must have the same number of elements). Taking $L=Id$, Theorem \ref{thm:matrep} yields the equation
\begin{equation}\label{eqn:basechange}
{}_{S_2}({\bf v}) = {}_{S_2}(Id*{\bf v}) = {}_{S_2}Id_{S_1}*{}_{S_1}{\bf v}
\end{equation}
where 
\begin{equation}\label{eqn:basechangematrix}
{}_{S_2}Id_{S_1} = [{}_{S_2}{\bf v}_1\ {}_{S_2}{\bf v}_2\ \dots\ {}_{S_2}{\bf v}_n]
\end{equation}
The matrix ${}_{S_2}Id_{S_1}$ is referred to as a {\it base transition matrix}, and written as ${}_{S_2}T_{S_1}$. In words, equations (\ref{eqn:basechange}) and (\ref{eqn:basechangematrix}) tells us that {\it in order to compute the coordinate vector ${}_{S_2}{\bf v}$ from ${}_{S_1}{\bf v}$, we multiply ${}_{S_1}{\bf v}$ on the left by the $n\times n$ matrix whose $i^{th}$ column is the coordinate vector of ${\bf v}_i$ with respect to the basis $S_2$}.
\vskip.2in

\begin{theorem} Suppose $S_i,1\le i\le 3$ are three bases for $V$. Then one has the following equalities
\begin{itemize}
\item ${}_{S_3}T_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}$
\item ${}_{S_i}T_{S_i} = Id$
\item ${}_{S_i}T_{S_j} = \left({}_{S_j}T_{S_i}\right)^{-1}$
\end{itemize}
\end{theorem}

\begin{exercise} Verify these three properties (notice that the second and third properties are closely related, in light of the first. Note also that the third property verifies that base transition matrices are always non-singular).
\end{exercise}

\begin{example} [to be included]

\end{example}

\begin{exercise} Let $S_1,S_2$ be two bases for $V$, and $L:V\to V$ a linear transformation from $V$ to itself. We can consider The representations ${}_{S_1}L_{S_1}$ and ${}_{S_2}L_{S_2}$ of $L$ with respect to the bases $S_1$ and $S_2$. Using the above identities, show that
\[
{}_{S_1}L_{S_1} = A*{}_{S_2}L_{S_2}*A^{-1}
\]
where $A = {}_{S_1}T_{S_2}$.
\end{exercise}

Square matrices $B,C$ which satisfy the equality $B = A*C*A^{-1}$ are called {\it similar}. This is an important relation between square matrices, and plays a prominent role in the theory of eigenvalues and eigenvectors.
\vskip.5in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inner product spaces}

\subsection{The dot product in $\mathbb R^n$} Consider $\mathbb R^n$ equipped with its standard basis (so that vectors in $\mathbb R^n$ are canonically identified with their $n\times 1$ coordinate representations). Then given ${\bf v} = [v_1\ v_2\ \dots\ v_n], {\bf w} = [w_1\ w_2\ \dots\ w_n]^T\in\mathbb R^n$, their {\it dot product} (also referred to as {\it scalar product}) is given by
\[
{\bf v}\cdot{\bf w} := {\bf v}^T*{\bf w} = \sum_{i=1}^n v_iw_i
\]

This operation on pairs of vectors satisfies three basic properties

\begin{enumerate}
\item[(IP1)] It is symmetric:
\[
{\bf v}\cdot {\bf w} = {\bf w}\cdot {\bf v}
\]
\item[(IP2)] It is bilinear:
\begin{gather*}
(\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2)\cdot {\bf w} = \alpha_1{\bf v}_1\cdot{\bf w} + \alpha_2{\bf v}_2\cdot{\bf w}\\
{\bf v}\cdot (\beta_1{\bf w}_1 + \beta_2{\bf w}_2) = \beta_1{\bf v}\cdot{\bf w}_1 + \beta_2{\bf v}\cdot{\bf w}_2
\end{gather*}
\item[(IP3)] It is positive non-degenerate:
\[
{\bf v}\cdot {\bf v}\ge 0;\quad {\bf v}\cdot {\bf v} = 0\,\text{ iff } {\bf v} = {\bf 0}
\]
\end{enumerate}
\vskip.2in

Note that the standard {\it Euclidean norm} of $\bf v$  (also referred to as the $\ell^2$-norm) is closely related to the dot product; precisely
\begin{equation}
\|{\bf v}\| = \|{\bf v}\|_2 = \sqrt{v_1^2 + v_2^2 +\dots v_n^2} = ({\bf v}\cdot {\bf v})^{\frac12}
\end{equation}

Its essential features are

\begin{enumerate}
\item[(N1)] It is positive definite:
\[
\|{\bf v}\|\ge 0;\quad \|{\bf v}\| = 0 \text{ iff } {\bf v} = 0
\]
\item[(N2)] It satisfies the triangle inequality (for norms):
\[
\|{\bf v} + {\bf w}\|\le \|{\bf v}\| + \|{\bf w}\|
\]
\end{enumerate}
\vskip.2in

This is the norm used in $\mathbb R^n$ to define the standard Euclidean {\it metric}, which is the conventional way to measure the distance between two vectors:
\begin{equation}
d({\bf v},{\bf w}) := \|{\bf v} - {\bf w}\|_2
\end{equation}

Again, this distance function - or metric - satisfies three basic properties, which are direct consequences of the ones above.

\begin{enumerate}
\item[(M1)] It is symmetric:
\[
d({\bf v},{\bf w}) = d({\bf w},{\bf v})
\]
\item[(M2)] It is positive non-degenerate:
\[
d({\bf v},{\bf w})\ge 0\,\,\forall {\bf v}, {\bf w}\in\mathbb R^n;\text{ moreover } d({\bf v},{\bf w}) = 0\,\text{ iff } {\bf v} = {\bf w}
\]
\item[(M3]) It satisfies the triangle inequality (for metrics):
\[
d({\bf u},{\bf w})\le d({\bf u},{\bf v}) + d({\bf v},{\bf w})\quad\forall {\bf u}, {\bf v}, {\bf w}\in\mathbb R^n
\]
\end{enumerate}
\vskip.2in

So the i) dot product, ii) Euclidean norm, and iii) Euclidean distance are all closely related. In fact, any one of them determines the other two. Obviously, the dot product determines the norm, and the norm determines the distance. But also one has

\begin{itemize} 
\item the equality
\[
{\bf v}\cdot {\bf w} = \frac12(\|{\bf v} + {\bf w}\|^2 - \|{\bf v}\|^2 - \|{\bf w}\|^2)
\]
so that one can also recover the dot product from the norm;
\item $\|{\bf v}\| = d({\bf v}, {\bf 0})$, so $d(_-,_-)$ and $\|_-\|$ determine each other.
\end{itemize}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Symmetric bilinear pairings on $\mathbb R^n$, and their representation} The dot product defined in the previous section is a specific example of a {\it bilinear, symmetric pairing}. We consider these properties in sequence.
\vskip.2in

A {\it bilinear pairing} on $\mathbb R^n$ is a map $P:\mathbb R^n\times\mathbb R^n\to \mathbb R$ which simply satisfies property (IP2). A {\it symmetric bilinear pairing} is a bilinear pairing that also satisfies (IP1). These pairings admit a straightforward matrix representation, not unlike the matrix representation of linear transformations discussed previously. Again, we assume we are looking at coordinate vectors with respect to the standard basis for $\mathbb R^n$.

\begin{theorem}\label{thm:matrep} For any bilinear pairing $P$ on $\mathbb R^n$, there is a unique $n\times n$ matrix $A_P$ such that
\[
P({\bf v},{\bf w}) = {\bf v}^T*A_P*{\bf w}
\]
Moreover, if $P$ is symmetric then so is $A_P$. Conversely, any $n\times n$ matrix $A$, determines a unique bilinear pairing $P_A$ on $\mathbb R^n$ by
\[
P_A({\bf v}, {\bf w}) = {\bf v}^T*A*{\bf w}
\]
which is symmetric precisely when $A$ is.
\end{theorem}

\begin{proof} Because it is bilinear, $P$ is uniquely characterized by its values on ordered pairs of basis vectors; moreover two bilinear pairings $P, P'$ are equal precisely if $P({\bf e}_i,{\bf e}_j) = P'({\bf e}_i,{\bf e}_j)$ for all pairs $1\le i,j\le n$ . So define $A_P$ be the $n\times n$ matrix with $(i,j)^{th}$ entry given by
\[
A_P(i,j) := P({\bf e}_i,{\bf e}_j),\quad 1\le i,j\le n
\]
By construction, the pairing $({\bf v},{\bf w})\mapsto {\bf v}^T*A_P*{\bf w}$ is bilinear, and agrees with $P$ on ordered pairs of basis vectors. Thus the two agree everywhere. This establishes a 1-1 correspondence (bilinear pairings on $\mathbb R^n$) $\Leftrightarrow$ ($n\times n$ matrices). Again, by construction, the matrix $A_P$ will be symmetric  iff $P$ is. Thus this correspondence restricts to a 1-1 correspondence (symmetric bilinear pairings on $\mathbb R^n$) $\Leftrightarrow$ ($n\times n$ symmetric matrices).
\end{proof}

\begin{definition} An {\it inner product} on $\mathbb R^n$ is a symmetric bilinear pairing $P$ that is also positive definite:
\[
P({\bf v}, {\bf v})\ge 0;\quad P({\bf v}, {\bf v}) = 0\,\text{ iff } {\bf v} = {\bf 0}
\]
\end{definition}

In other words, it also satisfies property (IP3). However, unlike properties (IP1) and (IP2), (IP3) is harder to translate into properties of the representing matrix. In fact, this last property will only come into focus after we have covered eigenspaces and diagonalizability for symmetric matrices, which is done below.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Orthogonal vectors and subspaces in $\mathbb R^n$} The concept of orthogonality is dependent on the choice of inner product. So assume first that we are working with the standard dot product in $\mathbb R^n$. We say two vectors $\bf v$, $\bf w$ are {\it orthogonal} if they are non-zero and ${\bf v}\cdot{\bf w} = 0$; we indicate this by writing ${\bf v}\perp {\bf w}$. Orthogonality with respect to this standard inner product corresponds to our usual notion of {\it perpendicular} (as we shall see below). More generally, a collection of non-zero vectors $\{{\bf v}_i\}$ is said to be orthogonal if they are pairwise orthogonal; in other words, ${\bf v}_i\cdot{\bf v}_j = 0$ for all $i\ne j$.
\vskip.2in

The notion of orthogonality extends to subspaces. Thus if $V,W\subset\mathbb R^n$ are two non-zero subspaces, we say $V$ and $W$ are {\it orthogonal} ($V\perp W$) if ${\bf v}\cdot{\bf w} = 0\,\,\forall{\bf v}\in V, {\bf w}\in W$. As with a collection of vectors, a collection of subspaces $\{V_i\}$ is orthogonal iff it is pairwise orthogonal: $V_i\perp V_j\,\,\forall i\ne j$.
\vskip.2in

\begin{example} Let $V = Span\{[2\,\, 3]^T\}, W = Span\{[ 3\,\, (-2)]^T\}\subset \mathbb R^2$. Then it is easy to check that $V$ and $W$ are orthogonal (geometrically, they are represented by two lines in $\mathbb R^2$ passing through the origin and forming a $90^\circ$ angle between them).
\end{example}
\vskip.2in

If $W\subset\mathbb R^n$ is a subspace, its {\it orthogonal complement} is given by $W^\perp := \{{\bf v}\in\mathbb R^n\ |\ {\bf v}\cdot{\bf w} = 0\,\forall {\bf w}\in W\}$. $W^\perp$ is the largest subspace of $\mathbb R^n$ for which every non-zero vector in the subspace is orthogonal to every non-zero vector in $W$. 
\vskip.2in

\begin{exercise} Show that for any subspace $W$, $W = \left(W^\perp\right)^\perp$.
\end{exercise}
\vskip.2in
Orthogonality is connected to the property of linear independence.

\begin{lemma} If $\{{\bf v}_1,\dots,{\bf v}_m\}$ is an orthogonal set of vectors in $\mathbb R^n$, then it is linearly independent.
\end{lemma}

\begin{proof} Suppose there exist scalars $\alpha_i$ with $\alpha_1{\bf v}_1 +\dots \alpha_m{\bf v}_m = {\bf 0}$. Then for each $i$ one has
\[
{\bf 0} = {\bf v}_i\cdot{\bf 0} = {\bf v}_i\cdot (\alpha_1{\bf v}_1 +\dots \alpha_m{\bf v}_m) = \alpha_i({\bf v}_i\cdot{\bf v}_i)
\]
which implies $\alpha_i=0$ as ${\bf v}_i\cdot{\bf v}_i = \|{\bf v}_i\|^2 > 0$.
\end{proof}
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Orthonormal vectors and orthogonal matrices} An orthogonal set of vectors $\{{\bf u}_1, {\bf u}_2,\dots,{\bf u}_n\}$ is said to be {\it orthonormal} if $\|{\bf u}_i\| = 1, 1\le i\le n$. Clearly, given an orthogonal set of vectors $\{{\bf v}_1, {\bf v}_2,\dots,{\bf v}_n\}$, one can orthonormalize it by setting ${\bf u}_i = {\bf v}_i/\|{\bf v}_i\|$ for each $i$. Orthonormal bases in $\mathbb R^n$ ``look" like the standard basis, up to rotation of some type.
\vskip.2in

We call an $n\times n$ matrix $A$ {\it orthogonal} if the columns of $A$ form an orthonormal set of vectors\footnote{One might expect such a matrix to be called orthonormal.}.

\begin{exercise} Show that an $n\times n$ matrix $A$ is orthogonal iff $A^T*A = I$.
\end{exercise}

\begin{lemma} An $n\times n$ matrix $A$ is orthogonal iff 
\[
{\bf v}\cdot{\bf w} = (A*{\bf v})\cdot (A*{\bf w})
\]
for all ${\bf v}, {\bf w}\in\mathbb R^n$.
\end{lemma}

\begin{proof} By Theorem \ref{thm:matrep}, we see that two matrices $A,B$ satsify the property
\[
{\bf v}^T*A^T*A*{\bf w} = (A*{\bf v})\cdot (A*{\bf w}) = (B*{\bf v})\cdot (B*{\bf w}) = {\bf v}^T*B^T*B*{\bf w}\qquad\forall {\bf v}, {\bf w}\in\mathbb R^n
\]
iff $A^T*A=B^T*B$. The hypothesis of the lemma can be restated as 
\[
(A*{\bf v})\cdot (A*{\bf w}) = (I*{\bf v})\cdot (I*{\bf w})\qquad\forall {\bf v}, {\bf w}\in\mathbb R^n
\]
implying $A^T*A=I^T*I = I$, which by the previous exercise is equivalent to $A$ being orthogonal.
\end{proof}

The notion of orthogonality for matrices is a special example of a linear transformation preserving a given inner product, which we will discuss in more detail below.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Projections onto subspaces and Gram-Schmidt orthogonalization} We know that every non-zero vector space admits a basis. It is natural then to ask: {\it does every non-zero inner product space admit an orthogonal basis}? The answer is: yes, it does. In fact, given a basis for an inner product space, there is a systematic way to convert it into an orthogonal basis. And this method simultaneously provides a method for projecting a vector onto a subspace. Again, we discuss the procedure first for $\mathbb R^n$ equipped with its standard scalar product, then show how this naturally extends to more general inner product spaces.
\vskip.2in

\subsubsection{Projection onto 1-dimensional subspaces} Suppose $W = Span\{{\bf v}\}$ is a 1-dimensional subspace of $\mathbb R^n$ (so that ${\bf v}\ne {\bf 0}$). Then given ${\bf w}\in\mathbb R^n$, we define the {\it projection of $\bf w$ onto $W$} to be
\[
pr_W({\bf w}) := \left(\frac{{\bf v}\cdot {\bf w}}{{\bf v}\cdot{\bf v}}\right) {\bf v}
\]
Note that this quantity makes sense, as ${\bf v}\ne {\bf 0}$ implies ${\bf v}\cdot{\bf v} > 0$.

\begin{proposition} The vector $pr_W({\bf w})$ depends only on the vector $\bf w$ and the subspace $W$. In other words, it does not depend on the choice of basis for $W$.
\end{proposition}

\begin{proof} Any other basis vector for $W$ can be written as $\alpha{\bf v}$ for some $\alpha\ne 0$. Then
\[
\left(\frac{\alpha{\bf v}\cdot {\bf w}}{\alpha{\bf v}\cdot\alpha{\bf v}}\right) {\alpha\bf v} = \left(\frac{\alpha^2}{\alpha^2}\right)\left(\frac{{\bf v}\cdot {\bf w}}{{\bf v}\cdot{\bf v}}\right) {\bf v} = \left(\frac{{\bf v}\cdot {\bf w}}{{\bf v}\cdot{\bf v}}\right) {\bf v}
\]
\end{proof}

The vector $pr_W({\bf w})$ represents {\it the $W$-component of ${\bf w}$} (in texts, this projection is also referred to as the {\it component of $\bf w$ in the direction of $\bf v$}. We prefer the subspace interpretation, as it makes clear the independence on the choice of basis element).
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Gram-Schmidt orthogonalization} Let $\{{\bf v}_1,{\bf v}_2,\dots,{\bf v}_n\}$ be a basis for $\mathbb R^n$. We will give an inductive procedure for constructing an orthogonal basis for $\mathbb R^n$ from this original set.
\vskip.2in

First, some notation. Let $W_i := Span\{{\bf v}_1,\dots,{\bf v}_i\}$ be the span of the first $i$ vectors in the set. Since any subset of linearly independent vectors is linearly independent, we see that $Dim(W_i) = i, 1\le i\le n$, with $W_n = \mathbb R^n$. 
\vskip.2in

Now $\{{\bf v}_1\}$ is an orthogonal basis for $W_1$, since it has only one element. We set ${\bf u}_1 = {\bf v}_1$, and consider the vector ${\bf v}_2$. This need not be orthogonal to ${\bf v}_1$, but it cannot be simply a scalar multiple of ${\bf v}_1$ either, since that would imply that the set $\{{\bf v}_1, {\bf v}_2\}$ was linearly dependent, contradicting what we know.
\vskip.2in

So we define
\[
{\bf u}_2 := {\bf v}_2 - pr_{W_1}({\bf v}_2)
\]

As we have just observed, ${\bf u}_2\ne {\bf 0}$.

\begin{exercise} Compute the dot product ${\bf u}_1\cdot{\bf u}_2$, and confirm it is zero. Also, verify that $Span(\{{\bf u}_1,{\bf u}_2\}) = Span(\{{\bf v}_1,{\bf v}_2\})$. Conclude that $\{{\bf u}_1,{\bf u}_2\}$ is an orthonormal basis for $W_2$.
\end{exercise}
\vskip.2in

We now suppose that we have constructed an orthogonal basis $\{{\bf u}_1,\dots,{\bf u}_m\}$ for $W_m$. We need to show how to this can be extended to $W_{m+1}$ if $m<n$. First, for ${\bf v}\in\mathbb R^n$, we define the projection of ${\bf v}$ onto $W_m$ to be
\[
pr_{W_m}({\bf v}) := \sum_{i=1}^m \left(\frac{{\bf u}_i\cdot {\bf v}}{{\bf u}_i\cdot{\bf u}_i}\right){\bf u}_i\in W_m
\]

Again, if ${\bf v}\notin W_m$, then ${\bf v}\ne pr_{W_m}({\bf v})$ and so their difference will not be zero. As abov, we then set
\[
{\bf u}_{m+1} = {\bf v}_{m+1} - pr_{W_m}({\bf v}_{m+1})
\]
 The same arguments used in the previous exercise show

\begin{proposition} If $m < n$, then $\{{\bf u}_1,\dots,{\bf u}_m,{\bf u}_{m+1}\}$ is an orthogonal basis for $W_{m+1}$.
\end{proposition}

Continuing in this fashion, we eventually reach the case $m=n$ at which point the algorithm is complete. Note that this procedure depends not only on the basis but also on the {\it order} in which we list the basis elements - changing the order will (most of the time) result in a different orthogonal basis for $\mathbb R^n$. Note also that this procedure works just as well if we start with a subspace of $\mathbb R^n$, together with a basis for that subspace. Summarizing

\begin{theorem} For any subspace $W$ of $\mathbb R^n$ and basis $S = \{{\bf v}_1,\dots,{\bf v}_m\}$ for that subspace, the Gram-Schmidt algorithm produces an orthogonal basis $\{{\bf u}_1,\dots,{\bf u}_m\}$ for $W$, which depends only on the ordering of the initial basis elements in $S$. Given this orthogonal basis for $W$ and an arbitrary vector ${\bf v}\in\mathbb R^n$, the projection of $\bf v$ onto $W$, or the $W$-component of $\bf v$ is given by
\[
pr_W({\bf v}) := \sum_{i=1}^m \left(\frac{{\bf u}_i\cdot {\bf v}}{{\bf u}_i\cdot{\bf u}_i}\right){\bf u}_i
\] 
\end{theorem}

The reason for calling this projection the $W$-component of $bf v$ is more or less clear, since the equation
\[
{\bf v} = pr_W({\bf v}) + ({\bf v} - pr_W({\bf v}))
\]
decomposes $bf v$ as a sum of i) its component in $W$, and ii) its component in $W^\perp$. As we have seen above, $\mathbb R^n = W\oplus W^\perp$, so this sum decomposition of $\bf v$ is {\it unique}. In other words,

\begin{corollary} For any non-zero subspace $W\subset\mathbb R^n$, $pr_W({\bf v})$ is the unique vector in $W$ for which the difference ${\bf v} - pr_W({\bf v})$ lies in $W^\perp$.
\end{corollary}

As we are about to see,  this is equivalent to saying that it is the vector in $W$ {\it closest} to $\bf v$, where distance is in terms of the standard Euclidean distance for $\mathbb R^n$ based on the scalar product.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Least-squares approximations} The setup is as before: we are given a subspace $W\subset\mathbb R^n$ and a vector ${\bf v}\in\mathbb R^n$. The questions are then:
\begin{itemize}
\item[(LS1)] Is there a vector ${\bf w}_{\bf v}\in W$ satisfying the property that $\|{\bf w}_{\bf v} - {\bf v}\|\le \|{\bf w} - {\bf v}\|$ for all ${\bf w}\in W$?
\item[(LS2)] If such a vector exists, is it unique?
\end{itemize}

As one might imagine from the previous section, the answer is ``yes". More precisely, the vector in $W$ we are looking for is exactly the projection of $\bf v$ onto $W$:

\begin{proposition} ${\bf w}_{\bf v} = pr_W({\bf v})$.
\end{proposition}

\begin{proof} Write ${\bf v} - pr_W({\bf v})$ as ${\bf w}_m$. Let ${\bf w}\in W$ be an arbitrary vector, and let ${\bf w}_p := pr_W({\bf v}) - {\bf w}$. Then ${\bf w}_m\cdot{\bf w}_p = 0$, so one has
\[
\|{\bf v} - {\bf w}\|^2 = \|{\bf w}_m + {\bf w}_p\|^2 = ({\bf w}_m + {\bf w}_p)\cdot ({\bf w}_m + {\bf w}_p) = {\bf w}_m\cdot{\bf w}_m + {\bf w}_p\cdot{\bf w}_p = \|{\bf w}_m\|^2 + \|{\bf w}_p\|^2\ge \|{\bf w}_m\|^2
\]
This shows that the projection satisfies (LS1). However we have already shown that the projection of $\bf v$ onto $W$ is unique, so (LS2) follows as well.
\end{proof}
\vskip.2in

The vector ${\bf v}_m = pr_W({\bf v})$ is referred to as {\it the least-squares approximation of ${\bf v}$ by a vector in $W$}, because $pr_W({\bf v})$ satisfies the property that $\|{\bf v}-pr_W({\bf v})\|^2$, which is computed as a sum of squares of differences in coordinates,  is minimized. This turns out to have an important application to finding the best approximation to a system of equations in the event no actual solution exists. We discuss this next.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Least-squares solutions and the Fundamental Subspaces theorem} Suppose we are given a matrix equation $A*{\bf x} = {\bf b}$ with $\bf x$ a vector variable taking values in $\mathbb R^n$, and $\bf b$ a fixed vector in $\mathbb R^m$ (implying that $A$ is an $m\times n$ matrix). The consistency theorem for systems of equations tells us that the equation is consistent precisely when $\bf b$ is in the span of the columns of $A$, or alternatively, when ${\bf b}\in C(A)$. But what if it is not? In other words, the system is inconsistent? Up until now we have simply left it at that; inconsistency was the end of the story.
\vskip.2in

But it is not. Because whether or not the original system is consistent, one can always find a solution to the related equation
\begin{equation}\label{eqn:lss}
A*{\bf x} = pr_{C(A)}({\bf b})
\end{equation}
because the projection $pr_{C(A)}({\bf b})$ of $\bf b$ onto the column space of $A$ will always be in the column space of $A$ regardless of whether or not the original vector $\bf b$ is.
\vskip.2in

The question then becomes: given that we know (\ref{eqn:lss}) has at least one solution, how do we go about finding it (or them)? The starting point for answering that question is the following theorem, often referred to as the Fundamental Subspaces theorem (originally proven by Gauss)

\begin{theorem}\label{thm:fst} For any $m\times n$ matrix $A$, there are equalities
\begin{itemize}
\itemsep.01in
\item $C(A)^\perp = N(A^T)$
\item $C(A) = N(A^T)^\perp$
\item $C(A^T)^\perp = N(A)$
\item $C(A^T) = N(A)^\perp$
\end{itemize}
\end{theorem}


\begin{proof} Because the theorem is stated for all matrices, and because $(W^\perp)^\perp = W$ for any subspace $W$, the second, third and fourth statements are consequences of the first, and is suffices to verify that case. To see this, we recall that $C(A)$ is the subspace of $\mathbb R^m$ spanned by the columns of $A$; then ${\bf v}\in C(A)^\perp$ iff $A(:,i)\cdot {\bf v} = 0$ for all $1\le i\le n$ iff $A^T(i,:)*{\bf v} = A(:,i)^T*{\bf v} = 0$ for all $1\le i\le n$ iff ${\bf v}\in N(A^T)$.
\end{proof}
\vskip.2in

Write ${\bf b}'$ for $pr_{C(A)}({\bf b})$. Then
\begin{itemize}
\itemsep.005in
\item[] $A*{\bf x} = {\bf b}'$
\item[$\Leftrightarrow$] $A*{\bf x} -{\bf b}\in C(A)^\perp$ (as ${\bf b}'$ is the unique vector in $C(A)$ with ${\bf b}' - {\bf b}\in C(A)^\perp$)
\item[$\Leftrightarrow$] $A*{\bf x} -{\bf b}\in N(A^T)$ (by Theorem \ref{thm:fst})
\item[$\Leftrightarrow$] $ A^T*A*{\bf x} - A^T*{\bf b} = A^T*(A*{\bf x} -{\bf b}) = {\bf 0}$
\item[$\Leftrightarrow$] $A^T*A*{\bf x} = A^T*{\bf b}$
\end{itemize}
\vskip.2in

This last equation $A^T*A*{\bf x} = A^T*{\bf b}$ has the same set of solutions as the equation that started the sequence, namely $A*{\bf x} = {\bf b}'$, and is therefore {\it always consistent}. It is derived from our original equation $A*{\bf x} = {\bf b}$ by simply multiplying both sides on the left by $A^T$, and is often referred to as the {\it associated normal equation} of the original matrix equation from which it was derived.
\vskip.2in

This yields a straightforward procedure for finding the {\it least-squares solution} to our original equation $A*{\bf x} = {\bf b}$; i.e., a solution to the associated normal equation $A^T*A*{\bf x} = A^T*{\bf b}$, which by the above is equivalent to a solution to the related equation $A*{\bf x} = pr_{C(A)}({\bf b})$. Note that the original equation is consistent precisely when ${\bf b}\in C(A)$, or equivalently when ${\bf b} = pr_{C(A)}({\bf b})$; in other words, when the {\it least-squares solution is an exact solution}. The advantages to seeking a least-squares solution are i) it always exists (regardless of whether or not the original equation is consistent), and ii) it yields an actual solution whenever an actual solutions exist. Because this procedure finds the least-squares solution {\it first}, it can be also applied to finding the least-squares approximation to $\bf b$ as $pr_{C(A)}({\bf b}) = A*{\bf x}$, where $\bf x$ is a least-squares solution to the original equation. 
\vskip.2in

The steps are:
\begin{itemize}
\item[(Step 1)] Form the associated normal equation $A^T*A*{\bf x} = A^T*{\bf b}$;
\item[(Step 2)] find the solution(s) to the normal equation by computing $rref([A^T*A\ |\ A^T*{\bf b}])$. These will be the least-squares solution(s) to the original equation;
\item[(Step 3)] for any least-squares solution ${\bf x}$ from Step 2, compute $A*{\bf x}$. This will yield the least-squares approximation $pr_{C(A)}({\bf b})$ to $\bf b$ by a vector in the column space of $A$.
\end{itemize}
\vskip.2in

Again, there will only be {\it one} least-squares approximation to $\bf b$ by a vector in $C(A)$, because we have already seen such a vector is unique. However, the set of least-squares solutions to the original equation may not be unique. Thus another consequence of this theory is

\begin{corollary} The value of $A*{\bf x}$ remains constant as $\bf x$ ranges over the set of least-squares solutions to the matrix equation $A*{\bf x} = {\bf b}$.
\end{corollary}

A final question then remains; when will there be a {\it unique} least-squares solution? We say that the matrix $A$ has {\it full column rank} (or just {\it full rank} when there is no confusion) if the columns of $A$ are linearly independent; namely that $rank(A) = n$. If $A$ is $m\times n$, this imposes the constraint that $m\ge n$ (otherwise the rank would have to be less than $n$ the number of columns). A useful fact about the ranks of matrices (which we do not prove here) is

\begin{lemma} For any matrix $A$, $rank(A) = rank(A^T*A)$. In particular, $A$ has full column rank iff $A^T*A$ is non-singular.
\end{lemma}

As the normal equation is always consistent, we see

\begin{corollary} $A^T*A*{\bf x} = A^T*{\bf b}$ will have a unique solution precisely when $N(A^T*A) = \{{\bf 0}\}$, which happens iff $A^T*A$ is non-singular. In this case, the unique least-squares solution is given by
\[
{\bf x} = (A^T*A)^{-1}*A^T*{\bf b}
\]
and the least-squares approximation to $\bf b$ by a vector in the column space of $A$ is
\[
pr_{C(A)}({\bf b}) = A*{\bf x} = A*(A^T*A)^{-1}*A^T*{\bf b}
\]
\end{corollary}
\vskip.3in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Applications of least-squares solutions} There are many important applications of least-squares. We present two.

\subsubsection{Projection onto a subspace} The corollary stated at the end of the previous section indicates an alternative, and more computationally efficient method of computing the projection of a vector onto a subspace $W$ of $\mathbb R^n$. Previously we had to first establish an orthogonal basis for $W$. But given any basis $\{{\bf v}_1,\dots,{\bf v}_m\}$ for $W$, we can avoid first orthogonalizing the basis by

\begin{itemize}
\item Concatenating the basis vectors to form the matrix $A$ with $A(:,i) = {\bf v}_i,1\le i\le m$,
\item then for any vector ${\bf v}\in\mathbb R^n$, computing the projection of $\bf v$ onto $W = C(A)$ as
\[
pr_W({\bf v}) = A*(A^T*A)^{-1}*A^T*{\bf v}
\]
\end{itemize}

\begin{exercise} If $A$ has maximal rank, verify that $B = A*(A^T*A)^{-1}*A^T$ satisfies the identity $B*B = B$ (a matrix satisfying such an identity is called a {\it projection matrix}, since the linear transformation it defines on $\mathbb R^n$ corresponds exactly to projection onto its range $C(B)$).
\end{exercise}
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Polynomial data fitting} We suppose given $n$ points $\{(x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)\}$ in the plane $\mathbb R^2$, with distinct $x$-coordinates (in practice, such sets of points can arise as data based on the measurement of some quantity - recorded as the $y$-coordinate - as a function of some parameter recorded as the $x$-coordinate). Then we would like to find the equation of the line that {\it best fits} these points (by exactly what measurement the line represents a best possible fit is explained below). if we write the equation of the line as $y = l(x) = c_0 + c_1x$ for indeterminants $c_0, c_1$, then what we are looking for is a {\it least-squares solution} to the $n\times 2$ system of equations
\begin{align*}
c_0 + c_1 x_1 &= y_1\\
c_0 + c_1 x_2 &= y_2\\
&\vdots\\
c_0 + c_1 x_n &= y_n
\end{align*}

Note that, in this system, the $x_i$ and $y_j$ are constants, and we are trying to solve for $c_0$ and $c_1$. For $n\le 2$ there will be a solution, but in the overdetermined case there almost always fails to be one. Hence the need to work in the least-squares setting.
\vskip.2in

\begin{example} We wish to find the least-squares fit by a linear equation to the set of points $(2,3), (4,6), (7,10), (9,14)$. This problem can be represented by the matrix equation
\[
A1*{\bf c} = {\bf y}
\]
Where $A1 = \begin{bmatrix} 1 & 2\\1 & 4\\1 & 7\\1 & 9\end{bmatrix}$, ${\bf c} = \begin{bmatrix}c_0\\c_1\end{bmatrix}$, and ${\bf y} = \begin{bmatrix} 3\\6\\10\\14\end{bmatrix}$. We note that this matrix is full rank. Therefore least-squares solution is unique and given by
\[
{\bf c} = (A1^T*A1)^{-1}*A1^T*{\bf y} = \begin{bmatrix}-.18966\\ 1.53448\end{bmatrix}
\]
Thus the desired equation is given by
\[
l(x) = -.18966 + 1.53448 x
\]
We can also measure the degree to which this comes close to being an actual solution (which would only exist if the points were colinear). Given $\bf c$, the vector
\[
{\bf y}_1 := A1*{\bf c} = \begin{bmatrix} 2.8793\\ 5.9483\\ 10.5517\\ 13.6207\end{bmatrix}
\]
is (by the above) the least-squares approximation to $\bf y$ by a vector in the column space of $A1$ (accurate to 4 decimal places). The accuracy can then be estimated by the distance of this approximation to the original vector $\bf y$:
\[
e_1 := \|{\bf y} - {\bf y}_1\| = 0.68229
\]
\end{example}

The last computation in this example indicates what is being minimized when one fits data points in this way. 

\begin{remark} Using least-squares linear approximation techniques to find the best linear fit to a set of $n$ data points $\{(x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)\}$ results in the equation of a line $l(x) = c_0 + c_1(x)$ which minimizes the sum of the squares of the {\it vertical} distances from the given points to the line:
\[
\sum_{i=1}^n (y_i - l(x_i))^2
\]
Note that, unless the line is horizontal, the vertical distance will be slightly larger than the {\it actual} distance, which is measured in the direction orthogonal to the line, and minimizing the sum of squares of those distances would correspond geometrically to what one might normally think of as constituting a least-squares fit. However, the computation needed to find the best fit with respect to this sum is quite a bit more involved,. This linear algebraic approach provides a simple and efficient method for finding a good approximation by a line which will be exact whenever the points are colinear.
\end{remark}

The setup above provides a method for finding not just linear approximations, but higher order ones as well. The linear algebra is essentially the same. To illustrate,

\begin{example} Suppose instead we were asked to find the least-squares fit by a {\it quadratic} equation to the same set set of points $(2,3), (4,6), (7,10), (9,14)$. As before, this problem can be represented by the matrix equation
\[
A2*{\bf c} = {\bf y}
\]
Where $A2 = \begin{bmatrix} 1 & 2 & 4\\1 & 4 & 16\\1 & 7 & 49\\1 & 9 & 81\end{bmatrix}$, ${\bf c} = \begin{bmatrix}c_0\\c_1\\c_2\end{bmatrix}$, and ${\bf y} = \begin{bmatrix} 3\\6\\10\\14\end{bmatrix}$. We note that the matrix $A_2$ is again full rank (it has rank 3). Therefore least-squares solution is unique and given by
\[
{\bf c} = (A2^T*A2)^{-1}*A2^T*{\bf y} = \begin{bmatrix} 0.960345\\ 0.984483\\0.050000\end{bmatrix}
\]
Thus the desired equation is given by
\[
q(x) = 0.960345 + 0.984483 x + 0.050000 x^2
\]
Measuring the degree to which this comes close to being an actual solution (which would only exist if the points all lay on the same quadratic graph), we compute
\[
{\bf y}_2 := A2*{\bf c} = \begin{bmatrix} 3.1293\\ 5.6983\\ 10.3017\\ 13.8707\end{bmatrix}
\]
is (by the above) the least-squares approximation to $\bf y$ by a vector in the column space of $A2$ (accurate to 4 decimal places). The accuracy can then be estimated by the distance of this approximation to the original vector $\bf y$:
\[
e_2 := \|{\bf y} - {\bf y}_2\| = 0.46424
\]
As with the linear fit, the quantity being minimized is the sum of squares of vertical distances of the original points to the graph of this quadratic function. Notice the modest improvement; from $0.68229$ to $0.46424$. Because the column space of $A2$ contains the columns space of $A1$, the least-squares approximation ${\bf y}_2$ has to be at least as good as the linear one ${\bf y}_1$, and almost always will be closer to the original vector $\bf y$.
\end{example}

We will illustrate our final point by looking at what happens if we go one degree higher.

\begin{example} We will find the least-squares fit by a {\it cubic} equation to the same set set of points $(2,3), (4,6), (7,10), (9,14)$. As before, this problem can be represented by the matrix equation
\[
A3*{\bf c} = {\bf y}
\]
Where $A3 = \begin{bmatrix} 1 & 2 & 4\\1 & 4 & 16\\1 & 7 & 49\\1 & 9 & 81\\8 & 64 & 343 & 729\end{bmatrix}$, ${\bf c} = \begin{bmatrix}c_0\\c_1\\c_2\\c_3\end{bmatrix}$, and ${\bf y} = \begin{bmatrix} 3\\6\\10\\14\end{bmatrix}$. The matrix $A_3$ is still full rank (it has rank 34). Therefore least-squares solution is unique and given by
\[
{\bf c} = (A3^T*A3)^{-1}*A3^T*{\bf y} = \begin{bmatrix} -1.6\\ 2.890476\\-0.342857\\0.023810\end{bmatrix}
\]
Thus the desired equation is given by
\[
f(x) = -1.6 + 2.890476 x + -0.342857 x^2 + 0.023810 x^3
\]
However, now when we compute the least-squares approximation we get
\[
{\bf y}_3 := A3*{\bf c} = \begin{bmatrix} 3\\ 6\\ 10\\ 14\end{bmatrix}
\]
which is not just an approximation but rather the vector $\bf y$ on the nose; $e_3 = 0$.. In other words, given these four points, {\it there is a unique cubic equation which fits the points exactly}. Inspecting the computation more carefully, we see why: the matrix $A3$ is both full rank and {\it square}. In other words, non-singular. In this case the system of equations is no longer over determined but rather balanced. And with a non-singular coefficient matrix, we get a unique solution. Symbolically, this can be seen by noting that the non-singularity of $A3$ results in a simplified expression for $\bf c$, confirming it is indeed an exact solution:
\[
{\bf c} = (A3^T*A3)^{-1}*A3^T*{\bf y} = A3^{-1}*(A3^T)^{-1}*A3^T*{\bf y} = A3^{-1}*y
\]
\end{example}

This set of examples, in which we compute successively higher order approximations to a set of $n$ data points until we finally arrive at an exact fit, is part of a more general phenomenon, which we record without proof by the following theorem.

\begin{theorem} Given $n$ points in $\mathbb R^2$ with distinct $x$-coordinates $\{(x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)\}$, the least-squares fit by a polynomial of degree $k$ is computed by finding the least-squares solution to the matrix equation
\[
A_k*{\bf c} = {\bf y}
\]
where ${\bf y} = [y_1\ y_2\ \dots y_n]$ and  $A_k$ is the $n\times (k+1)$ matrix with $A_k(i,j) = x_i^{j-1}$. The matrix $A_k$ will have full column rank for all $k\le (n-1)$, and so the least-squares solution $\bf c$ is unique and given by
\[
[c_0\ c_1\ \dots c_k]^T = {\bf c} = (A_k^T*A_k)^{-1}*A_k^T*{\bf y}
\]
with degree $k$ polynomial least-squares fit given by
\[
p_k(x) = \sum_{i=0}^k c_i x^i
\]
Because $A_{n-1}$  is non-singular, there will be a polynomial of degree at most $(n-1)$ which fits the points exactly. Moreover, the polynomial of degree at most $(n-1)$ which accomplishes this will be unique.
\end{theorem}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{The complex scalar product in $\mathbb C^n$} We consider first the analogue of the scalar, or dot product for $\mathbb C^n$. Recall first that if $[z_1\ z_2\dots z_n] = {\bf v}\in\mathbb C^n$, then the {\it conjugate} of $\bf v$ is the vector that results from applying complex conjugation degreewise 
\[
\ov{\bf v} := [\ov{z_1}\ \ov{z_2}\ \dots \ov{z_n}]
\]
Then the scalar product for $\mathbb C^n$ is given by
\[
{\bf v}\cdot {\bf w} := \ov{\bf w}^T*{\bf v}
\]

The properties for this pairing differ slightly than the corresponding ones for the real case, for reasons that will become clear shortly. They are

\begin{enumerate}
\item[(HIP1)] It is conjugate-symmetric:
\[
{\bf v}\cdot {\bf w} = \ov{{\bf w}\cdot {\bf v}}
\]
\item[(HIP2)] It is linea in the first variable and conjugate linear in the second:
\begin{gather*}
(\alpha_1{\bf v}_1 + \alpha_2{\bf v}_2)\cdot {\bf w} = \alpha_1{\bf v}_1\cdot{\bf w} + \alpha_2{\bf v}_2\cdot{\bf w}\\
{\bf v}\cdot (\beta_1{\bf w}_1 + \beta_2{\bf w}_2) = \ov{\beta_1}{\bf v}\cdot{\bf w}_1 + \ov{\beta_2}{\bf v}\cdot{\bf w}_2
\end{gather*}
\item[(HIP3)] It is positive non-degenerate:
\[
\mathbb R\ni {\bf v}\cdot {\bf v}\ge 0;\quad {\bf v}\cdot {\bf v} = 0\,\text{ iff } {\bf v} = {\bf 0}
\]
\end{enumerate}
\vskip.2in

\begin{remark} It is the third property that accounts for the need to use $\ov{\bf w}^T*{\bf v}$ rather than ${\bf w}^T*{\bf v}$. In fact, this need already exists for $\mathbb C = \mathbb C^1$. For if $z = a + bi$ is a complex number, its {it norm} is computed as $\sqrt{a^2 + b^2} = \sqrt{z\cdot\ov{z}}$.
\end{remark}

An inner product on a complex vector space satisfying these three properties is usually referred to as a {\it Hermitian} inner product, the one just defined for $\mathbb C^n$ being the {\it standard} Hermitian inner product, or complex scalar product.
\vskip.2in

 As in the real case, the {\it norm} of ${\bf v}\in\mathbb C^n$  (also referred to as the $\ell^2$-norm) is closely related to the complex scalar product; precisely
\begin{equation}
\|{\bf v}\| = \|{\bf v}\|_2 =  ({\bf v}\cdot {\bf v})^{\frac12}
\end{equation}

This norm on complex $n$-space satisfies the same two properties as before:

\begin{enumerate}
\item[(N1)] It is positive definite:
\[
\|{\bf v}\|\ge 0;\quad \|{\bf v}\| = 0 \text{ iff } {\bf v} = 0
\]
\item[(N2)] It satisfies the triangle inequality (for norms):
\[
\|{\bf v} + {\bf w}\|\le \|{\bf v}\| + \|{\bf w}\|
\]
\end{enumerate}
\vskip.2in

This norm on $\mathbb C^n$ defines the standard {\it metric}, which is the standard way to measure the distance between two vectors in complex $n$-space:
\begin{equation}
d({\bf v},{\bf w}) := \|{\bf v} - {\bf w}\|
\end{equation}

This distance function - or metric - satisfies the same three basic properties as it does in the real case:

\begin{enumerate}
\item[(M1)] It is symmetric:
\[
d({\bf v},{\bf w}) = d({\bf w},{\bf v})
\]
\item[(M2)] It is positive non-degenerate:
\[
d({\bf v},{\bf w})\ge 0\,\,\forall {\bf v}, {\bf w}\in\mathbb C^n;\text{ moreover } d({\bf v},{\bf w}) = 0\,\text{ iff } {\bf v} = {\bf w}
\]
\item[(M3]) It satisfies the triangle inequality (for metrics):
\[
d({\bf u},{\bf w})\le d({\bf u},{\bf v}) + d({\bf v},{\bf w})\quad\forall {\bf u}, {\bf v}, {\bf w}\in\mathbb C^n
\]
\end{enumerate}
\vskip.2in

So the i) complex scalar product, ii) standard complex norm, and iii) complex distance are all related; moreover as in the real case the norm and distance functions determine one another. However, in the complex case it is no longer true that one can recover the inner product from the norm. It is only the real part that can be expressed this way:
\[
Re({\bf v}\cdot{\bf w}) = \frac12({\bf w}\cdot{\bf v} + {\bf v}\cdot {\bf w}) = \frac12(\|{\bf v} + {\bf w}\|^2 - \|{\bf v}\|^2 - \|{\bf w}\|^2)
\]
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Conjugate-symmetric sesquilinear pairings on $\mathbb C^n$, and their representation} The complex scalar product defined in the previous section is a specific example of a {\it sesquilinear, conjugate-symmetric pairing}. We consider these properties in sequence.
\vskip.2in

A {\it sesquilinear pairing} on $\mathbb C^n$ is a map $P:\mathbb C^n\times\mathbb C^n\to \mathbb C$ which satisfies property (HIP2), namely it is linear in the first variable and conjugate linear in the second\footnote{this is the standard convention in mathematical literature. Many physics texts, however, reverse this, making the first variable conjugate linear instead.}. A {\it conjugate-symmetric sesquilinear pairing} is a sesquilinear pairing that also satisfies (HIP1). These pairings admit a matrix representation as in the real case discussed above. Again, we assume we are looking at coordinate vectors with respect to the standard basis for $\mathbb C^n$.
\vskip.2in

Before stating the result, we need to record

\begin{definition} For a complex matrix $A$, the {\it conjugate-transpose of $A$} is $A^* = \ov{A}^T$; $A^*(i,j) = \ov{A(j,i)}$. An $n\times n$ complex matrix is {\it Hermitian} if $A^* = A$.
\end{definition}

\begin{theorem}\label{thm:matrepcomp} For any sesquilinear pairing $P$ on $\mathbb C^n$, there is a unique $n\times n$ matrix $A_P$ such that
\[
P({\bf v},{\bf w}) = \ov{\bf w}^T*A_P*{\bf v} = {\bf w}^**A_P*{\bf v}
\]
Moreover, if $P$ is conjugate-symmetric then $A_P$ is Hermitian. Conversely, any $n\times n$ matrix $A$, determines a unique sesquilinear pairing $P_A$ on $\mathbb R^n$ by
\[
P_A({\bf v}, {\bf w}) = \ov{\bf w}^T*A*{\bf v} = {\bf w}^**A_P*{\bf v}
\]
which is conjugate-symmetric precisely when $A$ is Hermitian.
\end{theorem}

\begin{proof} The proof is essentially the same as in the real case, with some minor modifications. $P$ is uniquely characterized by its values on ordered pairs of basis vectors; moreover two bilinear pairings $P, P'$ are equal precisely if $P({\bf e}_i,{\bf e}_j) = P'({\bf e}_i,{\bf e}_j)$ for all pairs $1\le i,j\le n$ . So define $A_P$ be the $n\times n$ matrix with $(i,j)^{th}$ entry given by
\[
A_P(j,i) := P({\bf e}_i,{\bf e}_j),\quad 1\le i,j\le n
\]
By construction, the pairing $({\bf v},{\bf w})\mapsto \ov{\bf w}^T*A_P*{\bf v}$ is sesquilinear, and agrees with $P$ on ordered pairs of basis vectors. Thus the two agree everywhere. This establishes a 1-1 correspondence (sesquilinear pairings on $\mathbb C^n$) $\Leftrightarrow$ ($n\times n$ complex matrices). By construction, the matrix $A_P$ will be conjugate-symmetric  iff $P^T = \ov{P}$, or equivalently if $P$ is Hermitian. Thus this correspondence restricts to a 1-1 correspondence (conjugate-symmetric sesquilinear pairings on $\mathbb C^n$) $\Leftrightarrow$ ($n\times n$ Hermitian matrices).
\end{proof}

\begin{definition} A {\it Hermitian inner product} on $\mathbb C^n$ is a conjugate-symmetric sesquilinear pairing $P$ that is also positive definite:
\[
P({\bf v}, {\bf v})\ge 0;\quad P({\bf v}, {\bf v}) = 0\,\text{ iff } {\bf v} = {\bf 0}
\]
\end{definition}

In other words, it also satisfies property (HIP3). As in the real case, proper understanding of this last property will require a discussion of eigenspaces and diagonalizability for symmetric - and more generally Hermitian - matrices.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Unitary matrices} A set of $n$ vectors in $\mathbb C^n$ is {\it orthogonal} if it is so with respect to the standard complex scalar product, and {\it orthonormal} if in addition each vector has norm 1. Similarly, one has the complex analogue of a matrix being orthogonal.

\begin{definition} An $n\times n$ complex matrix $U$ is {\it unitary} if $U^**U = I$, or equivalently if $U^{-1} = U^*$.
\end{definition}

Just as orthogonal matrices are exactly those that preserve the dot product, we have

\begin{lemma} A complex $n\times n$ matrix is unitary iff
\[
{\bf w}^**{\bf v} = {\bf v}\cdot {\bf w} = (U*{\bf v})\cdot (U*{\bf w})\qquad\forall {\bf v},{\bf w}\in\mathbb C^n
\]
\end{lemma}

\begin{proof} Essentially the same as in the real case; by Theorem \ref{thm:matrepcomp} of the previous section we see that the hypothesis on $U$ implies $U^**U = I^**I = I$.
\end{proof}

Unitary matrices are special examples of linear transformations which preserve Hermitian inner products. More on this below.
\vskip.5in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Eigenvalues and eigenvectors}

\subsection{Definition} If $A$ is an $m\times n$ matrix, $\bf v$ an $n\times 1$ non-zero vector, we say that $\bf v$ is {\it an eigenvector of A with eigenvalue $\lambda$} if one has the identity
\[
A*{\bf v} = \lambda{\bf v}
\]
In other words, if multiplying $\bf v$ on the left by the matrix $A$ has the same effect as multiplying it by the scalar $\lambda$. We note first that, in order for this to be at all  possible, $A*{\bf v}$ must also be an $n\times 1$ vector; in other words, $A$ must be a {\it square} matrix.
\vskip.2in

Given a square matrix, then, the {\it eigenvalue problem} is to find a complete description of the eigenvalues and associated eigenvectors for that matrix. Our goal in this section is to determine a systematic way of doing this.
\vskip.2in

Before proceeding with examples, we note that

\begin{proposition} If $\bf v$ is an eigenvalue of a matrix $A$, the eigenvector associated with it is unique.
\end{proposition}

\begin{proof} Suppose $\lambda_1{\bf v} = A*{\bf v} = \lambda_2{\bf v}$. Then $\lambda_1{\bf v} - \lambda_2{\bf v} = (\lambda_1 - \lambda_2){\bf v} = {\bf 0}$. But since ${\bf v}\ne {\bf 0}$, the only way this could happen is if the coefficient $(\lambda_1 - \lambda_2)$ is equal to zero, or equivalently, if $\lambda_1 = \lambda_2$.
\end{proof}
\vskip.2in

\begin{example} Let $A = I^{n\times n}$ be the $n\times n$ identity matrix. Then for any ${\bf v}\in\mathbb R^n$, one has $A*{\bf v} = I*{\bf v} = {\bf v} = 1{\bf v}$. So in this case, we see that {\it every non-zero vector in $\mathbb R^n$ is an eigenvector of $A$ with corresponding eigenvalue $1$}.
\end{example}

[Additional examples to be included here]
\vskip.2in

In order to understand more clearly what it is we are looking for, we consider a reformulation of the defining equation above. First, we note that the scalar product $\lambda{\bf v}$ can be rewritten as a matrix product
\[
\lambda{\bf v} = (\lambda I)*{\bf v}
\]
From this we have the following equivalent statements:
\[
A*{\bf v} = \lambda{\bf v}\quad\Leftrightarrow\quad A*{\bf v} = (\lambda I)*{\bf v}\quad\Leftrightarrow\quad (A - \lambda I)*{\bf v} = {\bf 0}\quad\Leftrightarrow\quad {\bf v}\in N(A - \lambda I)
\]

Thus

\begin{observation}\label{obs:eigen} A non-zero vector $\bf v$ is an eigenvector of $A$ with eigenvalue $\lambda$ if and only if it lies in the nullspace of the matrix $A-\lambda I$. In particular, in order for such an vector to exist, the matrix $A-\lambda I$ must be singular.
\end{observation}
\vskip.2in

This suggests that in order to solve the eigenvalue problem, we should first determine the values $\lambda$ for which $A - \lambda I$ is singular; then, for each such $\lambda$, determine a basis for the nullspace of $A - \lambda I$. In other words, {\it eigenvalues first, then eigenvectors}. And to do this we will need an effective tool for determining when a square matrix is singular. This brings us to the determinant, discussed in the next section.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The determinant} The determinant of a matrix exists whenever the matrix is square, and whenever entries can be added and multiplied. Thus determinants exist for more than just matrices of numbers; for example, a square matrix whose entries are real-valued functions has a well-defined determinant.
\vskip.2in

There are different ways to define it. We present two.
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Cofactor expansion} If $A$ is an $n\times n$ matrix, with $n > 1$, we define the $(i,j)^{th}$ minor of $A$ to be $M_{ij}(A)$ to be the $(n-1)\times(n-1)$ matrix derived from $A$ by deleting the $i^{th}$ row and $j^{th}$ column. For example,
\vskip.2in

[include example here]
\vskip.2in
 In this framework one proceeds with an {\it inductive} or {\it recursive} definition. In such a definition, we give an explicit formula in the case $n=1$, and that prior to defining the determinant for $n\times n$ matirices, that the determinant {\it has already been given for $(n-1)\times(n-1)$ matrices}. For indices $1\le i,j\le n$, define the $(i,j)^{th}$ cofactor of $A$ to be
\[
A_{ij} = (-1)^{i+j}Det(M_{ij}(A))
\]
Then

\begin{definition} Id $A = [a]$ is a $1\times 1$ matrix, then $Det(A) = a$. For $n > 1$, $Det(A) = \sum_{j=1}^n A(1,j)A_{1j}$
\end{definition}

This is sometimes also referred to as {\it cofactor expansion along the first row}.
\vskip.2in
\begin{example} Suppose A = $\begin{bmatrix} 2 & 3\\1 & -4\end{bmatrix}$. Then $M_{11}(A) = [-4], M_{12}(A) = [1]$, so $A_{11} = (-1)^{1+1}(-4) = -4$ and $A_{12} = (-1)^{1+2}(1) = -1$. Then $Det(A) = A(1,1)A_{11} + A(1,2)A_{12} = (2)(-4) + (3)(-1) = -11$.
\end{example}
\vskip.2in

More generally, 

\begin{example} Suppose $A = \begin{bmatrix} a_{11} & a_{12}\\a_{21} & a_{22}\end{bmatrix}$. Then $M_{11}(A) = [a_{22}], M_{12}(A) = [a_{21}]$, so $A_{11} = (-1)^{1+1}(a_{22}) = a_{22}$ and $A_{12} = (-1)^{1+2}(a_{21}) = a_{21}$. Then 
\[
Det(A) = A(1,1)A_{11} + A(1,2)A_{12} = a_{11}a_{22} - a_{12}a_{21}
\]
\end{example}
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Combinatorial definition} Let $S_n = \{1,2,\dots,n\}$ be the set consisting of the intergers between $1$ and $n$ inclusive. We denote by $\Sigma_n$ the set of maps $\sigma:S_n\xrightarrow{\cong} S_n$ of $S_n$ to itself which are {\it isomorphisms}, meaning that they are 1-1 and onto. An element $\sigma\in\Sigma_n$ should be thought of as a {\it reordering} of the elements of $S_n$, with $\Sigma_n$ consisting of all such reorderings. Elements of $\Sigma_n$ can be {\it multiplied} (with multiplication given by composition), and with respect to that multiplication every element $\sigma\in\Sigma_n$ has an inverse $\sigma^{-1}\in\Sigma_n$ satisfying $\sigma \sigma^{-1} = Id$. A set satisfying these properties is called a {\it group}, and $\Sigma_n$ is typically referred to as {\it the permutation group on n letters}.
\vskip.2in
Among the elements of $\Sigma_n$ are permutations of a particularly simple type, called {\it transpositions}. The permutation which switches two numbers $i$ and $j$, while leaving all others fixed, will be labeled $\tau_{ij}$ (note: $\tau_{ij}$ is often written as $(i,j)$, however this can be confused with the coordinate notation for points in $\mathbb R^2$, hence our choice not to use it).
\vskip.2in
It is not hard to see that any permutation $\sigma\in\Sigma_n$ can be written as a product of transpositions:
\[
\sigma = \tau_{i_1,j_1}\tau_{i_2,j_2}\dots\tau_{i_m,j_m}
\]
The way of doing so is far from unique, but it turns out that - given $\sigma$ - the {\it number} of transpositions used rewriting $\sigma$ in this fashion is {\it always even or always odd}. This allows for the definition of the {\it sign} of a permutation:

\begin{definition} For $\sigma\in\Sigma_n$, the sign of $\sigma$, denoted by $sgn(\sigma)$ is given by
\[
sgn(\sigma) := (-1)^m\quad\text{if } \sigma = \tau_{i_1,j_1}\tau_{i_2,j_2}\dots\tau_{i_m,j_m}
\]
where the product on the right is a product of transpositions.
\end{definition}

With this concept established, the determinant may alternatively be defined as

\begin{definition} For an $n\times n$ matrix $A$, 
\[
Det(A) = \sum_{\sigma\in\Sigma_n}sgn(\sigma) A(1,\sigma(1))A(2,\sigma(2))\dots A(n,\sigma(n))
\]
\end{definition}
\vskip.2in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Properties of the determinant} The determinant satisfies a number of useful properties, among them

\begin{itemize}
\item For any two square matrices $A,B$ of the same dimensions, $Det(A*B) = Det(A)Det(B)$.
\item $A$ is singular iff $Det(A) = 0$.
\item If $A$ is triangular (either upper or lower), then $Det(A) = A(1,1)A(2,2)\dots A(n,n) =$ the product of the diagonal entries.
\item $Det(A) = Det(A^T)$.
\end{itemize}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The characteristic polynomial} The object is to establish algebraic criteria for determining exactly when a real number can occur as an eigenvalue of $A$. This leads us to

\begin{definition} For an $n\times n$ matrix $A$, the {\it characteristic polynomial of $A$} is given by
\[
p_A(t) := Det(A - tI)
\]
\end{definition}
Note the matrix here is not strictly numerical. Precisely, $(A-tI)(i,j) = \begin{cases} A(i,j)\,\,\text{if } i\ne j\\A(i,i) - t\,\,\text{if }i=j\end{cases}$. However, the determinant of such a matrix (using either one of the two equivalent definitions given above) is still well-defined.
\vskip.2in
In Observation \ref{obs:eigen} we noted that $\lambda$ is an eigenvalue of $A$ iff $A-\lambda I$ is singular. By the properties of the determinant listed above, we see that $A-\lambda I$ is singular iff its determinant is equal to zero. In other words,

\begin{lemma} $\lambda$ is an eigenvalue of $A$ iff $p_A(\lambda) = 0$; that is, $\lambda$ is a root of the characteristic polynomial $p_A(t)$ of $A$.
\end{lemma}
\vskip.2in

\begin{example} Consider the matrix $A = \begin{bmatrix} 2 & 2\\1 & 3\end{bmatrix}$. Then $p_A(t) = Det\left(\begin{bmatrix} (2-t) & 3\\1 & (-4-t)\end{bmatrix}\right) = (2-t)(3-t) - 2 = t^2 - 5t + 4 = (t-1)(t-4)$, indicating that there are two eigenvalues; $\lambda = 1$ and $\lambda=4$.
\end{example}
\vskip.2in

\begin{example} Consider the matrix $A = \begin{bmatrix} 2 & -4\\1 & 3\end{bmatrix}$. Then $p_A(t) = Det\left(\begin{bmatrix} (2-t) & 3\\1 & (-4-t)\end{bmatrix}\right) = (2-t)(3-t) + 4 = t^2 - 5t + 10$. In this case the quadratic formula shows that there are no {\it real} roots. There are, however, complex roots (how exactly one handles this case is dicsussed below).
\end{example}
\vskip.2in

The following is an immediate consequence of the definition of $p_A(t)$, and basic properties of polynomials.

\begin{lemma} If $A$ is an $n\times n$ matrix, then $p_A(t)$ is a polynomial of degree $n$. Consequently, $A$ can have at most $n$ distinct eigenvalues.
\end{lemma}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Eigenspaces} As we saw above, $\lambda$ is an eigenvalue of $A$ iff $N(A-\lambda I)\ne 0$, with the non-zero vectors in this nullspace comprising the set of eigenvectors of $A$ with eigenvalue $\lambda$.

\begin{definition} The eigenspace of $A$ corresponding to an eigenvalue $\lambda$ is $E_\lambda (A) := N(A-\lambda I)$.
\end{definition}

\begin{exercise} Suppose $A = \begin{bmatrix} 2 & 0\\0 & 3\end{bmatrix}$. Show that $p_A(t) = (2-t)(3-t)$, so that the two possible eigenvalues of $A$ are $\lambda = 2$ and $\lambda = 3$. Then show the two corresponding eigenspaces for these eigenvalues are $E_2(A) = Span\{{\bf e}_1\}$, $E_3(A) = Span\{{\bf e}_2\}$.
\end{exercise}

Note that the dimension of the eigenspace corresponding to a given eigenvalue must be at least 1, since eigenspaces must contain non-zero vectors by definition.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Direct sum decomposition} A vector space $V$ can be written as a {\it direct sum} of two subspaces $W_1, W_2$ if
\begin{itemize}
\item Every ${\bf v}\in V$ can be written as ${\bf v} = {\bf w}_1 + {\bf w}_2$ for ${\bf w}_i\in W_i$;
\item $W_1\cap W_2 = \{{\bf 0}\}$.
\end{itemize}

Another way of expressing this is to say that every vector in $V$ can be written {\it uniquely} as a sum of i) a vector in $W_1$ and ii) a vector in $W_2$. If $V$ is a direct sum of $W_1$ and $W_2$, we write $V = W_1\oplus W_2$. 
\vskip.2in


A similar description applies more generally to an $m$-fold direct sum: $V$ is a direct sum of subspaces $W_i$, $1\le i\le m$ if
\begin{itemize}
\item Every ${\bf v}\in V$ can be written as ${\bf v} = \sum_{i=1}^m{\bf w}_i$ for ${\bf w}_i\in W_i$;
\item $W_1\cap (W_2\oplus W_3\oplus\dots \oplus W_n) = \{{\bf 0}\}$.
\end{itemize}

If such is the case, we write either $V = \bigoplus_{i=1}^m W_i$, or $V = W_1\oplus W_2\oplus\dots \oplus W_m$. 

\begin{exercise} Show that if $\{{\bf u}_1,\dots, {\bf u}_m\}$ is a linearly independent set of vectors in $W_1$, $\{{\bf v}_1,\dots, {\bf v}_n\}$ a linearly independent set of vectors in $W_2$, and $W_i\subset V$ with $W_1\cap W_2 = \{{\bf 0}\}$, then $\{{\bf u}_1,\dots, {\bf u}_m,{\bf v}_1,\dots, {\bf v}_n\}$ is linearly independent.
\end{exercise}
\vskip.2in

\begin{exercise} Suppose $W$ is a subspace of $R^n$ (equipped with its standard inner product). Show that $\mathbb R^n = W\oplus W^{\perp}$
\end{exercise}

Direct sum decomposition applies just as well to subspaces of $\mathbb R^n$. Define $E(A)\subset\mathbb R^n$ to be the supspace of $\mathbb R^n$ spanned by the eigenvectors of $A$ (this may or may not be all of $\mathbb R^n$). As we have seen, the number of distinct possible eigenvalues of $A$ is at most $n$ when $A$ is an $n\times n$ matrix. In particular, it is finite.

\begin{theorem} Given a real $n\times n$ matrix $A$, let $\lambda_1,\dots,\lambda_m$ be the distinct real eigenvalues of $A$. Then 
\[
E(A) = E_{\lambda_1}(A)\oplus E_{\lambda_2}(A)\oplus\dots\oplus E_{\lambda_m}(A)
\]
\end{theorem}

\begin{proof} For $m=1$ the statement is trivially true. So suppose that $m=2$, and ${\bf v}\in E_{\lambda_1}(A)\cap E_{\lambda_2}(A)$ with $\lambda_1\ne \lambda_2$. Then
\[
A*{\bf v} = \lambda_1{\bf v} = \lambda_2{\bf v}
\]
implying $(\lambda_1 - \lambda_2){\bf v} = {\bf 0}$. As $\lambda_1 - \lambda_2\ne 0$, this implies ${\bf v} = {\bf 0}$.
\vskip.2in

Inductively we can assume that the span of the union of the eigenspaces $\{E_{\lambda_i}(A)\}_{i=2}^m$ is the direct sum of these subspaces.  We wish to show that $E_{\lambda_1}(A)\cap (E_{\lambda_2}(A)\oplus\dots\oplus E_{\lambda_m}(A)) = {\bf 0}$. So let ${\bf v}_1\in E_{\lambda_1}(A)\cap (E_{\lambda_2}(A)\oplus\dots\oplus E_{\lambda_m}(A))$. Then 
\[
{\bf v}_1 = {\bf v}_2 + {\bf v}_3 +\dots + {\bf v}_m
\]
where $A*{\bf v}_i = \lambda_i{\bf v}_i, 1\le i\le m$. Multiplying the above equality on the left by $A$ gives
\[
\lambda_1({\bf v}_2 + {\bf v}_3 +\dots + {\bf v}_m) = \lambda_1{\bf v}_1 = A*{\bf v}_1 = A*({\bf v}_2 + {\bf v}_3 +\dots + {\bf v}_m) = \lambda_2{\bf v}_2 + \lambda_3{\bf v}_3 +\dots + \lambda_m{\bf v}_m
\]
Subtracting gives
\[
(\lambda_1 - \lambda_2){\bf v}_2 + (\lambda_1 - \lambda_3){\bf v}_3 +\dots + (\lambda_1 - \lambda_m){\bf v}_m = {\bf 0}
\]
As the eigenvalues are distinct, the coefficient $(\lambda_1 - \lambda_i)$ is non-zero for each $2\le i\le m$. Then
\[
{\bf v}_i = \left(\frac{-1}{(\lambda_1 - \lambda_i)}\right)\Big((\lambda_1 - \lambda_2){\bf v}_2  (\lambda_1 - \lambda_3){\bf v}_3 \dots + (\lambda_1 - \lambda_{i-1}){\bf v}_{i-1} + (\lambda_1 -\lambda_{i+1}){\bf v}_{i+1}\dots +(\lambda_1 - \lambda_m){\bf v}_m\Big)
\]
Induction allows us to assume
\[
E_{\lambda_i}(A)\cap\left(E_{\lambda_2}(A)\oplus E_{\lambda_3}(A)\oplus\dots\oplus E_{\lambda_{i-1}}(A)\oplus E_{\lambda_{i+1}}(A)\oplus\dots\oplus E_{\lambda_m}(A)\right) = {\bf 0}
\]
which together with the last equation implies ${\bf v}_i = {\bf 0}$ for all $2\le i\le m$. As ${\bf v}_1 = \sum_{i=2}^m {\bf v}_i$ we conclude finally that ${\bf v}_1 = {\bf 0}$. Since ${\bf v}_1$ was taken to be an arbitrary element of $E_{\lambda_1}(A)\cap (E_{\lambda_2}(A)\oplus\dots\oplus E_{\lambda_m}(A))$ this shows the intersection must be zero, completing the proof.
\end{proof}

In the case $E(A) = \mathbb R^n$, this yields a direct sum decomposition of $\mathbb R^n$ into eigenspaces of $A$, which is a very useful thing to know (in the cases it occurs). We investigate this question next.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Similarity and diagonalization} Two square matrices $A$ and $B$ are said to be {\it similar} if there exists an invertible matrix $S$ such that
\[
B = S*A*S^{-1}
\]
The matrix $S$ appearing in this equation is referred to as a {\it similarity matrix}. Similarity represents an important {\it equivalence relation} on the vector space of square matrices of a given dimension. A number of important aspects of a square matrix remain unchanged under this relation.

\begin{lemma} If $A$ and $B$ are similar, then $p_A(t) = p_B(t)$. In particular, $Det(A) = Det(B)$.
\end{lemma}

\begin{proof} Choose $S$ such that $B = S*A*S^{-1}$. Then the equality of the characteristic polynomials follows from the sequence of equalities
\[
\begin{split}
p_B(t) = Det(B - tI) &= Det(S*A*S^{-1} - tI)\\
&= Det(S*A*S^{-1} - S*(tI)*S^{-1})\\
&= Det(S*(A - tI)*S^{-1})\\
&= Det(S)*Det(A - tI)*Det(S^{-1})\\
&= Det(S)*Det(S)^{-1}*Det(A - tI) = Det(A - tI) = p_A(t)
\end{split}
\]
\end{proof}

Thus similar matrices have the same eigenvalues, occuring with the same multiplicity. Moreover, their eigenvectors are related.

\begin{exercise} If $B = S*A*S^{-1}$, and ${\bf v}$ is an eigenvector of $A$ with eigenvalue $\lambda$, show that $S*{\bf v}$ is an eigenvalue of $B$ with eigenvalue $\lambda$. More generally, slow that if $L_S:\mathbb R^n\to \mathbb R^n$ is the linear transformation $L_S({\bf w}) = S*{\bf w}$, show that $L_S$ induces an isomorphism on eigenspaces $L_S:E_{\lambda}(A)\xrightarrow{\cong} E_{\lambda}(B)$ for all eigenvalues $\lambda$.
\end{exercise}

In the previous section we considered the question: when does $\mathbb R^n$ decompose as a direct sum of eigenspaces of a matrix $A$? To answer this, we consider first the case when $A = D$ a diagonal matrix with $D(i,i) = \lambda_i, 1\le i\le n$. For such a matrix, each standard basis vector ${\bf e}_i$ is an eigenvector, as $D*{\bf e}_i = \lambda_i{\bf e}_i$ for each $1\le i\le n$. So for such a matrix one has an evident direct sum decomposition
\[
\mathbb R^n = \bigoplus_{\lambda} E_{\lambda}(A),\quad E_{\lambda}(A) = Span\{{\bf e}_i\ |\ \lambda_i = \lambda\}
\]
\vskip.2in

We say that $A$ is {\it diagonalizable} if $A$ is similar to a diagonal matrix.

\begin{theorem} Given $A$, the following statements are equivalent:
\begin{enumerate}
\item $\mathbb R^n$ has a basis consisting of eigenvectors of $A$.
\item $\mathbb R^n$ can be written as a direct sum of eigenspaces of $A$.
\item $A$ is diagonalizable.
\end{enumerate}
\end{theorem}

\begin{proof} Statements (1) and (2) are clearly equivalent. It will suffice then to show that statement (2) is equivalent to statement (3). Suppose first that $A$ is diagonalizable, in other words that there is a matrix $S$ such that $A = S*D*S^{-1}$. Multiplying both sides of this equation on the right by $S$ yields
\[
A*S = S*D
\]
The $i^{th}$ column of $A*S$ is $A*(S(:,i))$, while the $i^{th}$ column of $S*D$ is $\lambda_i S(:,i)$ where $\lambda_i = D(i,i)$ represents the $i^{th}$ diagonal element of $D$. In othe words, we have an equality
\[
A*S(:,i) = \lambda_i S(:,i),\quad 1\le i\le n
\]
implying the columns of $S$ are eigenvectors of $A$. The $n\times n$ matrix $S$ is invertible, so must have rank $n$. This means the  set of column vectors $\{S(:,1), S(:,2),\dots, S(:,n)\}$ are linearly independent, and therefore form a basis for $\mathbb R^n$.
\vskip.2in

On the other hand, if $\{{\bf v}_1,\dots, {\bf v}_n\}$ is a basis of $\mathbb R^n$ with $A*{\bf v}_i = \lambda_i {\bf v}_i, 1\le i\le n$, then concatenating the vectors in the basis forms a matrix 
\[
S := [{\bf v}_1\ {\bf v}_2\ \dots {\bf v}_n]
\]
whose $i^{th}$ column is the $i^{th}$ eigenvector ${\bf v}_i$. If we now define $D$ to be the diagonal matrix with $D(i,i) = \lambda_i$, then (as above) one has
\[
A*S = S*D
\]
By construction the set of columns of $S$ are linearly independent, and so $S$ is invertible. So we may multiply both sides of this last equation on the right by $S^{-1}$, yielding
\[
A = S*D*S^{-1}
\]
implying that $A$ is diagonalizable. This completes the proof.
\end{proof}

Many matrices are diagonalizable, but there are also many that are not. The following exercise illustrates a class of matrices that won't be.

\begin{exercise} Show that $A = \begin{bmatrix} 1 & 1\\0 & 1\end{bmatrix}$ is not diagonalizable, by i) finding the single eigenvalue $\lambda_1$ for the matrix, and then ii) showing $E_{\lambda_1}(A)$ has dimension 1 (and this cannot be all of $\mathbb R^2$).
\end{exercise}

Of course, this discussion so far has been concerned with the case of real matrices and real eigenvalues. Some matrices have no eigenvalues over the real numbers. To illustrate

\begin{example} Let $A = \begin{bmatrix} 0 & 1\\-1 & 0\end{bmatrix}$. Then charactericstic polynomial of $A$ is $p_A(t) = t^2 + 1$, which is an irreducible quadratic polynomial with no real roots. Thus $A$ has no eigenvectors in $\mathbb R^2$.
\end{example}

What should be done with such matrices? The answer is that even if one is primarily concerned with real matrices and working over the real numbers, there are cases where one needs to enlarge the set of scalars to $\mathbb C$. This is one of those cases.
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Complex eigenvalues and eigenvectors} 
All of the constructions we have done so far over $\mathbb R$ extend naturally to $\mathbb C$, with some slight adjustment for the case of inner products (we will discuss this in more detail below). For now, the main reason for considering complex numbers has to do with the factorization of polynomials. The key result one want to know (whose proof involves techniques well beyond the scope of linear algebra) is

\begin{theorem} {\rm ( The Fundamental Theorem of Algebra)} Any non-constant polynomial $p(z)$ with complex coefficients has a complex root. Consequently, any non-constant polynomial with real or complex coefficients can be factored over $\mathbb C$ into a product of linear terms
\[
p(z) = c(z - r_1)(z - r_2)\dots (z - r_n),\qquad c,r_1,r_2,\dots,r_n\in\mathbb C
\]
\end{theorem}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Geometric vs algebraic multiplicity} The identity matrix $\begin{bmatrix} 1 & 0\\0 & 1\end{bmatrix}$ is obviously diagonalizable (it is diagonal to start with), and $\mathbb R^2$ has a basis consisting of eigenvectors of $I$, namely the standard basis $\{{\bf e}_1, {\bf e}_2\}$. On the other hand, we have seen that $\begin{bmatrix} 1 & 1\\0 & 1\end{bmatrix}$ is not, even though it has the same characteristic polynomial as $I$. This example tells us, among other things, that the characteristic polynomial alone does not determine whether or not a given matrix is diagonalizable. As it turns out, this problem can be studied one eigenvalue at a time.
\vskip.2in

Let $A$ be an arbitrary $n\times n$ matrix, and $\lambda$ an eigenvalue of $A$. The {\it geometric multiplicity} of $\lambda$ is defined as
\[
m_g(\lambda) := Dim(E_{\lambda}(A))
\]
while its {\it algebraic multiplicity} is the mutiplicity of $\lambda$ viewed as a root of $p_A(t)$ (as defined in the previous section).

\begin{theorem}\label{thm:geoalg} For all square matrices $A$ and eigenvalues $\lambda$, $m_g(\lambda)\le m_a(\lambda)$. Moreover, this holds over both $\mathbb R$ and $\mathbb C$ (in other words, both for real matrices with real eigenvalues, or more generally complex matrices with complex eigenvalues)
\end{theorem}

The proof of this result will be deferred until later. For now we record an important consequence. Let $\lambda_1,\dots,\lambda_k$ denote the (distinct) eigenvalues of the $n\times n$ matrix $A$. Then (working over $\mathbb C$ if needed) we can write the characteristic polynomial of $A$ as
\[
p_A(t) = (-1)^n(t-\lambda_1)^{m_1}(t-\lambda_2)^{m_2}\dots (t-\lambda_k)^{m_k}
\]
where $m_i = m_a(\lambda_i$ is the algebraic multiplicity of $\lambda_i$. From this factorization, we see that the sum of the algebraic multiplicities must equal the degree of $p_A(t)$, which equals $n$:
\[
\sum_{i=1}^k m_a(\lambda_i) = \sum_{i=1}^k m_i = n
\]
By the above theorem we have

\begin{theorem} Over $\mathbb C$ the matrix $A$ is diagonalizable iff for each eigenvalue $\lambda$ one has $m_g(\lambda) = m_a(\lambda)$. If $A$ is a real matrix and $p_A(t)$ factors over $\mathbb R$ into a product of linear terms, then the same holds over $\mathbb R$. 
\end{theorem}

\begin{proof}We start with the complex case, as working over $\mathbb C$ guaranteees complete factorization of $p_A(t)$. By Theorem \ref{thm:geoalg}, $\sum_i m_g(\lambda_i)\le \sum_i m_a(\lambda_i) = n$; moreover, since $0\le m_g(\lambda_i)\le m_a(\lambda_i)$ for each $i$, we have that $\sum_i m_g(\lambda_i) = n$ iff $m_g(\lambda_i) = m_a(\lambda_i)$ for each $i$. But $\sum_i m_g(\lambda_i) = Dim(E(A)$ (over $\mathbb C$), and $A$ is diagonalizable iff $Dim(E(A)) = n$. This proves the result over $\mathbb C$.
\vskip.2in

If $A$ is a real matrix and the factorization of $p_A(t)$ over $\mathbb C$ yields only real eigenvectors, then $p_A(t)$ factors completely into linear terms over $\mathbb R$, and the above argument can be repeated in this case to arrive at the same conclusion over $\mathbb R$.
\end{proof}

An $n\times n$ matrix $A$ is called {\it defective} if the sum of the geometric multiplicities over $\mathbb C$ is strictly less than $n$. Our last theorem shows that this happens iff there is some eigenvalue $\lambda$ of $A$ for which $m_g(\lambda) < m_a(\lambda)$.
The {\it deficiency} of a particular eigenvalue is then represented by the difference $m_a(\lambda) - m_g(\lambda)$. This can be arbitrarily large, as the next exercise illustrates.

\begin{exercise} Let $T_n$ be the $n\times n$ matrix with $T(i,j) = \begin{cases} 1\quad\text{if } i\le j\\ 0\quad\text{otherwise}\end{cases}$. Show that the eigenvalue $\lambda = 1$ has algebraic multiplicity $n$, but geometric multiplicity $1$.
\end{exercise}

On the other hand, for almost all matrices (from a statistical point of view), factorization of $p_A(t)$ into linear terms leads to $n$ distinct eigenvalues, which therefore must each have multiplicity equal to 1. Since the geometric multiplicity of a given eigenvalue must be {\it at least} 1 (as the corresponding eigenspace must be non-zero), we see that {\it any eigenvalue with algebraic multiplicity 1 cannot be deficient}. Hence $\sum_i m_g(\lambda_i) = n$ in this case, hence

\begin{corollary} Any $n\times n$ matrix with $n$ distinct eigenvalues is diagonalizable over $\mathbb C$. If the matrix and its eigenvalues are all real, the same statement is true over $\mathbb R$.
\end{corollary}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Shur's Theorem} Recall that a complex matrix $U$ is unitary if $U^* = U$. 

\begin{theorem} Let $A$ be a complex $n\times n$ matrix. Then there is a unitary matrix $U$ and upper triangular matrix $T$ such that
\[
U^**A*U = T
\]
\end{theorem}

\begin{proof} By the Fundamental Theorem of Algebra $p_A(t)$ factors completely over $\mathbb C$ into linear terms. In particular, $A$ must have at least one eigenvalue.  Let $\lambda_1$ be an eigenvalue of $A$, and ${\bf v}_1$ a corresponding eigenvector with norm 1. Let $W_1 = Span\{{\bf v}_1\}^\perp$ be the orthogonal complement of the span of ${\bf v}_1$. Write $L_1:\mathbb C^n\to\mathbb C^n$ for the linear transformation ${\bf v}\mapsto A*{\bf v}$. Now consider the linear transformation
\[
L_2:W_1\inj\mathbb C^n\xrightarrow{L_1}\mathbb C^n\surj \mathbb C^n/Span\{{\bf v}_1\}\cong W_1
\]
By induction on dimension, we can assume the subspace $W_1\subset\mathbb C^n$ admits a complex orthonormal basis $\{{\bf v}_2,\dots, {\bf v}_n\}$ such that $L_2({\bf v}_i)\subset Span\{{\bf v}_2,\dots,{\bf v}_i\}, 2\le i\le n$. In other words, for each $2\le i\le n$ we have
\[
L_2({\bf v}_i) = \sum_{j=2}^i\alpha_{i,j}{\bf v}_j
\]
This means that the composition $L_2':W_1\inj\mathbb C^n\xrightarrow{L_1}\mathbb C^n$ on the vectors $\{{\bf v}_2,\dots, {\bf v}_n\}$ is given by
\[
L_1({\bf v}_i) = L_2'({\bf v}_i) = \sum_{j=1}^i\alpha_{i,j}{\bf v}_j
\]
for some choice of complex scalars $\alpha_{1,j}$. Setting $\alpha_{1,1} = \lambda_1$, we see that
\[
A*{\bf v}_i = \sum_{j=1}^i\alpha_{i,j}{\bf v}_j,\qquad 1\le i\le n
\]
Let $U = [{\bf v}_1\ {\bf v}_2\dots {\bf v}_n]$ be the concatenation of the vectors ${\bf v}_i, 1\le i\le n$ and $T$ the upper triangular matrix with $T(i,j) = \alpha_{i,j}, 1\le i\le j\le n$. Then $U$ is unitary, and last equation can then be written in matrix form as
\[
A*U = U*T
\]
Multiplying both sides on the left by $U^* = U^{-1}$ gives the desired equation
\[
U^**A*U = T
\]
\end{proof}

The main corrolary to Shur's theorem is 

\begin{corollary} If $A$ is an $n\times n$ Hermitian matrix, then $A$ is diagonalizable (i.e., $\mathbb C^n$ has a basis consisting of eigenvectors of $A$). Moreover, all of the eigenvalues of $A$ are real.
\end{corollary}

\begin{proof} By Shur's Theorem, there exists a unitary $U$ and upper triangular $T$ with $U^**A*U = T$. But $A$ is Hermitian, so $A = A^*$. Then
\begin{align*}
T^* &= \left(U^**A*U\right)^*\\
        &= U^**A^**U\\
        &= U^**A*U = T
\end{align*}
But as $T$ is upper triangular, the identity $T = T^*$ implies not only that $T$ is diagonal, but that the diagonal elements remain invariant under complex conjugation; i.e., are real numbers. Thus Shur's equation may be rewritten in this case as $U^**A*U = D$ where $D$ is a real diagonal matrix, or alternatively,
\[
A*U = U*D
\]
But this last equation implies $A*U(i,:) = \lambda_i U(i,:), 1\le i\le n$, where $\lambda_i = D(i,i)$. As the columns of $U$ are orthonormal, they form a basis for $\mathbb C^n$, implying the result.
\end{proof}

As a special case, when $A$ is a real $n\times n$ matrix, then it is Hermitian iff it is symmetric. Thus, viewing it as a complex matrix with purely real entries, we have

\begin{corollary} If $A$ is an $n\times n$ real symmetric matrix, then it is diagonalizable and all of its eigenvalues are real.
\end{corollary}

We are now in a position to return to the question of how to decide when a sesquilinear conjugate-symmetric (complex) pairing - which in the real case reduces to a symmetric bilinear pairing - actually represents an inner product; in other words, is positive definite.

\begin{theorem} Let $<_-,_->:\mathbb C^n\times\mathbb C^n\to \mathbb C, ({\bf v}, {\bf w})\mapsto <{\bf v}, {\bf w}>$ be a sesquilinear conjugate-symmetric pairing on $\mathbb C^n$, represented (with respect to the standard basis) by the Hermitian matrix $A$. Then $<_-,_->$ is positive-definite iff all of the eigenvalues of $A$ are positive (equivalently, iff $A$ is similar to a diagonal matrix $D$ with positive diagonal entries).
\end{theorem}

\begin{proof} Let $\{{\bf v}_1, {\bf v}_2, \dots, {\bf v}_n\}$ be an orthonormal basis for $\mathbb C^n$ consisting of eigenvectors of $A$, with $A*{\bf v}_i = \lambda_i{\bf v}_i, \lambda_i\in\mathbb R$ (this basis exists by the corollary above). If ${\bf v} = \sum\alpha_i{\bf v}_i$, then 
\[
<{\bf v}, {\bf v}> = {\bf v}^**A*{\bf v} = \sum_i|\alpha_i|^2\lambda_i
\]
It is easily seen that this sum satisfies condition (HIP3) iff $\lambda_i > 0\,\,\forall 1\le i\le n$.
\end{proof}
\vskip.2in

A slightly more general class of Hermitian matrices are those that are {\it non-negative definite}, meaning that their eigenvalues are all non-negative. Now suppose $A$ is an arbitrary $n\times n$ complex matrix, and $B = A^**A$. Then clearly $B^* = (A^**A)^* = A^**A = B$ implying $B$ is Hermitian.

\begin{lemma} For any $n\times n$ matrix $A$, the matrix $B = A^**A$ is non-negative definite. Moreover, $B$ is positive definite iff $A$ is non-singular.
\end{lemma}

\begin{proof} Since $\mathbb C^n$ admits an orthonormal basis consisting of eigenvectors of $B$, it suffices to show that the eigenvalues of $B$ are non-negative. Let $\bf v$ be an eigenvector of $B$ with eigenvalue $\lambda$. Then
\[
\lambda\|{\bf v}\|^2 = \lambda({\bf v}^**{\bf v}) = {\bf v}^**B*{\bf v} = {\bf v}^**(A^**A)*{\bf v} = (A*{\bf v})^**(A*{\bf v}) = \|A*{\bf v}\|^2\ge 0
\]
As $\|{\bf v}\|^2 > 0$ this implies $\lambda\ge 0$. Also, as we have already seen, $B$ is non-singular iff $0$ is not an eigenvalue of $B$. But $Det(B) = |Det(A)|^2$. Hence $B$ is non-singular iff $A$ is.
\end{proof}
\vskip.3in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Normal matrices} The identity $U^**A*U = T$ can be expressed as saying that $A$ is {\it unitarily similar} to the triangular matrix $T$ (in other words, the similarity matrix is not just invertible, but unitary). It is reasonable to ask whether of not a complex matrix $A$ is unitarily similar to a diagonal matrix, or alternatively, whether or not $\mathbb C^n$ admits an orthonormal basis consisting of eigenvectors of $A$.
\vskip.2in

\begin{definition} $A$ is {\it normal} if $A^**A = A*A^*$.
\end{definition}

\begin{lemma} $A$ is normal iff $A$ is unitarily diagonalizable.
\end{lemma}

\begin{proof} Shur's identity can be rewritten as $A = U*T*U^*$ ($U$ unitary and $T$ triangular). Then $A$ normal implies
\[
(U*T*U^*)*(U*T*U^*)^* = (U*T*U^*)^**(U*T*U^*)
\]
Recalling that $(C*D)^* = D^**C^*$, and that unitary means $U^**U = I$, the above identity simplifies to
\[
U*T*T^**U^* = U*T^**T*U^*\quad\Leftarrow\quad T*T^* = T^**T
\]
One can check that this last condition implies $T$ must be a diagonal matrix (in other words, the only triangular matrix which is also normal is a diagonal one. Note, though, that $T$ need not have real entries).
\vskip.2in

Conversely, if $T$ is diagonal, then $T$ is normal (as we just noted), and $A$ is unitarily similar to a normal matrix, which implies it too is normal.
\end{proof}
\vskip.5in

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Singular value decomposition} A reasonable question to ask is whether or not there is an identity that generalizes Shur's identity in the case of normal matrices. The answer is provided by the following theorem.

\begin{thm} (Singular Value Decomposition) Let $A$ be an $m\times n$ matrix. Then there exists an $m\times m$ unitary matrix $U$, $n\times n$ unitary matrix $V$, and $m\times n$ diagonal matrix $\Sigma$ with
\[
A = U*\Sigma*V
\]
\end{thm}

We will defer proof of this theorem to the latter part of this section. Our primary purpose will be to put the terminology in context, and investigate some of its more important applications (of which there are many).


\end{document}